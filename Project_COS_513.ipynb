{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec693ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import  accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f1089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset.\n",
    "dataset = pd.read_csv('winequalityN.csv', \n",
    "                      sep=',', na_values=[\"?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc07ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0  white            7.0              0.27         0.36            20.7   \n",
       "1  white            6.3              0.30         0.34             1.6   \n",
       "2  white            8.1              0.28         0.40             6.9   \n",
       "3  white            7.2              0.23         0.32             8.5   \n",
       "4  white            7.2              0.23         0.32             8.5   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.045                 45.0                 170.0   1.0010  3.00   \n",
       "1      0.049                 14.0                 132.0   0.9940  3.30   \n",
       "2      0.050                 30.0                  97.0   0.9951  3.26   \n",
       "3      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "4      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "\n",
       "   sulphates  alcohol  quality  \n",
       "0       0.45      8.8        6  \n",
       "1       0.49      9.5        6  \n",
       "2       0.44     10.1        6  \n",
       "3       0.40      9.9        6  \n",
       "4       0.40      9.9        6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the first 5 rows to get an idea of the dataset.\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52e727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6497 entries, 0 to 6496\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   type                  6497 non-null   object \n",
      " 1   fixed acidity         6487 non-null   float64\n",
      " 2   volatile acidity      6489 non-null   float64\n",
      " 3   citric acid           6494 non-null   float64\n",
      " 4   residual sugar        6495 non-null   float64\n",
      " 5   chlorides             6495 non-null   float64\n",
      " 6   free sulfur dioxide   6497 non-null   float64\n",
      " 7   total sulfur dioxide  6497 non-null   float64\n",
      " 8   density               6497 non-null   float64\n",
      " 9   pH                    6488 non-null   float64\n",
      " 10  sulphates             6493 non-null   float64\n",
      " 11  alcohol               6497 non-null   float64\n",
      " 12  quality               6497 non-null   int64  \n",
      "dtypes: float64(11), int64(1), object(1)\n",
      "memory usage: 660.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#information about the attributes and the variables of the dataset.\n",
    "dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf70ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6487.000000</td>\n",
       "      <td>6489.000000</td>\n",
       "      <td>6494.000000</td>\n",
       "      <td>6495.000000</td>\n",
       "      <td>6495.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6488.000000</td>\n",
       "      <td>6493.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.216579</td>\n",
       "      <td>0.339691</td>\n",
       "      <td>0.318722</td>\n",
       "      <td>5.444326</td>\n",
       "      <td>0.056042</td>\n",
       "      <td>30.525319</td>\n",
       "      <td>115.744574</td>\n",
       "      <td>0.994697</td>\n",
       "      <td>3.218395</td>\n",
       "      <td>0.531215</td>\n",
       "      <td>10.491801</td>\n",
       "      <td>5.818378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.296750</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>0.145265</td>\n",
       "      <td>4.758125</td>\n",
       "      <td>0.035036</td>\n",
       "      <td>17.749400</td>\n",
       "      <td>56.521855</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.160748</td>\n",
       "      <td>0.148814</td>\n",
       "      <td>1.192712</td>\n",
       "      <td>0.873255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.992340</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.994890</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.996990</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    6487.000000       6489.000000  6494.000000     6495.000000   \n",
       "mean        7.216579          0.339691     0.318722        5.444326   \n",
       "std         1.296750          0.164649     0.145265        4.758125   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.400000          0.230000     0.250000        1.800000   \n",
       "50%         7.000000          0.290000     0.310000        3.000000   \n",
       "75%         7.700000          0.400000     0.390000        8.100000   \n",
       "max        15.900000          1.580000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  6495.000000          6497.000000           6497.000000  6497.000000   \n",
       "mean      0.056042            30.525319            115.744574     0.994697   \n",
       "std       0.035036            17.749400             56.521855     0.002999   \n",
       "min       0.009000             1.000000              6.000000     0.987110   \n",
       "25%       0.038000            17.000000             77.000000     0.992340   \n",
       "50%       0.047000            29.000000            118.000000     0.994890   \n",
       "75%       0.065000            41.000000            156.000000     0.996990   \n",
       "max       0.611000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  6488.000000  6493.000000  6497.000000  6497.000000  \n",
       "mean      3.218395     0.531215    10.491801     5.818378  \n",
       "std       0.160748     0.148814     1.192712     0.873255  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.110000     0.430000     9.500000     5.000000  \n",
       "50%       3.210000     0.510000    10.300000     6.000000  \n",
       "75%       3.320000     0.600000    11.300000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     9.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5865d315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for duplicate rows\n",
    "dups = dataset.duplicated()\n",
    "dups.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738c331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497\n",
      "5329\n"
     ]
    }
   ],
   "source": [
    "#dropping the duplicate rows and checking the amount of rows after the drop\n",
    "print( (dataset.shape[0]))\n",
    "dataset = dataset.drop_duplicates()\n",
    "print((dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7c5ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4318\n",
       "1    1011\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the quality column into binary values, 1 for good (quality above or 7 ) and 0 for bad (quality bellow 7).\n",
    "dataset['quality'] = dataset['quality'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "quality_column =dataset['quality'].value_counts()\n",
    "quality_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "639932ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.029</td>\n",
       "      <td>29.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.98920</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>white</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045</td>\n",
       "      <td>43.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.99390</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>white</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.147</td>\n",
       "      <td>38.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.38</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.63</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.044</td>\n",
       "      <td>55.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.99740</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>white</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.46</td>\n",
       "      <td>10.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.30</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>white</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.029</td>\n",
       "      <td>18.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.99230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.52</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.31</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.051</td>\n",
       "      <td>33.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.99900</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>white</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.046</td>\n",
       "      <td>31.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.99290</td>\n",
       "      <td>3.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.14</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.037</td>\n",
       "      <td>18.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.99600</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.07</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.043</td>\n",
       "      <td>34.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.99440</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>white</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.044</td>\n",
       "      <td>34.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.99450</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.049</td>\n",
       "      <td>36.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.99310</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>white</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045</td>\n",
       "      <td>73.5</td>\n",
       "      <td>214.0</td>\n",
       "      <td>0.99340</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.61</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>white</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.028</td>\n",
       "      <td>24.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.59</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>20.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.99180</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>white</td>\n",
       "      <td>7.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.039</td>\n",
       "      <td>8.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.99420</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>white</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.034</td>\n",
       "      <td>37.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.99200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>white</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.083</td>\n",
       "      <td>50.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.99650</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>white</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.51</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.039</td>\n",
       "      <td>62.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.40</td>\n",
       "      <td>7.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.99280</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>white</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.055</td>\n",
       "      <td>9.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.99405</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>white</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.049</td>\n",
       "      <td>46.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.99850</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.53</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>white</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.35</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.039</td>\n",
       "      <td>38.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.99940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.42</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.29</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.046</td>\n",
       "      <td>29.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.99520</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.53</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>white</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.044</td>\n",
       "      <td>22.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.99014</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.036</td>\n",
       "      <td>38.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.99622</td>\n",
       "      <td>3.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>white</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.032</td>\n",
       "      <td>29.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.99298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>white</td>\n",
       "      <td>6.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6320</th>\n",
       "      <td>red</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.079</td>\n",
       "      <td>39.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.84</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6321</th>\n",
       "      <td>red</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.123</td>\n",
       "      <td>14.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>red</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.063</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.99444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>red</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.081</td>\n",
       "      <td>13.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.99631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6486</th>\n",
       "      <td>red</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.068</td>\n",
       "      <td>34.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99414</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.78</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>red</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "17    white            NaN             0.660         0.48             1.2   \n",
       "33    white            6.2             0.120         0.34             NaN   \n",
       "54    white            6.8             0.200         0.59             0.9   \n",
       "86    white            7.2               NaN         0.63            11.0   \n",
       "98    white            9.8             0.360         0.46            10.5   \n",
       "139   white            8.1             0.280         0.39             1.9   \n",
       "174   white            NaN             0.270         0.31            17.7   \n",
       "224   white            6.3             0.495         0.22             1.8   \n",
       "249   white            NaN             0.410         0.14            10.4   \n",
       "267   white            NaN             0.580         0.07             6.9   \n",
       "268   white            5.3             0.585          NaN             7.1   \n",
       "368   white            NaN             0.290         0.48             2.3   \n",
       "438   white            7.4             0.155         0.34             NaN   \n",
       "440   white            6.7             0.220         0.37             1.6   \n",
       "518   white            NaN             0.130         0.28             1.9   \n",
       "521   white            7.9               NaN         0.26             2.1   \n",
       "587   white            6.1             0.320         0.25             1.7   \n",
       "621   white            6.5               NaN         0.43             8.9   \n",
       "697   white            7.5             0.310         0.51            14.8   \n",
       "747   white            7.2             0.290         0.40             7.6   \n",
       "812   white            6.4               NaN         0.28             1.1   \n",
       "909   white            7.5             0.240          NaN            13.0   \n",
       "972   white            6.8             0.220         0.35            17.5   \n",
       "1079  white            NaN               NaN         0.29             6.2   \n",
       "2894  white            6.4               NaN         0.36             1.4   \n",
       "2902  white            NaN             0.360         0.14             8.9   \n",
       "4892  white            6.5             0.230         0.38             1.3   \n",
       "4895  white            6.5               NaN         0.19             1.2   \n",
       "6320    red            7.0             0.540          NaN             2.1   \n",
       "6321    red            6.4             0.530         0.09             3.9   \n",
       "6428    red            NaN             0.440         0.09             2.2   \n",
       "6429    red            NaN             0.705         0.10             2.8   \n",
       "6486    red            7.2               NaN         0.33             2.5   \n",
       "6493    red            5.9             0.550         0.10             2.2   \n",
       "\n",
       "      chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "17        0.029                 29.0                  75.0  0.98920  3.33   \n",
       "33        0.045                 43.0                 117.0  0.99390  3.42   \n",
       "54        0.147                 38.0                 132.0  0.99300   NaN   \n",
       "86        0.044                 55.0                 156.0  0.99740  3.09   \n",
       "98          NaN                  4.0                  83.0  0.99560  2.89   \n",
       "139       0.029                 18.0                  79.0  0.99230   NaN   \n",
       "174       0.051                 33.0                 173.0  0.99900  3.09   \n",
       "224       0.046                 31.0                 140.0  0.99290  3.39   \n",
       "249       0.037                 18.0                 119.0  0.99600  3.38   \n",
       "267       0.043                 34.0                 149.0  0.99440  3.34   \n",
       "268       0.044                 34.0                 145.0  0.99450  3.34   \n",
       "368       0.049                 36.0                 178.0  0.99310  3.17   \n",
       "438       0.045                 73.5                 214.0  0.99340  3.18   \n",
       "440       0.028                 24.0                 102.0  0.99130   NaN   \n",
       "518       0.050                 20.0                  78.0  0.99180  3.43   \n",
       "521       0.039                  8.0                 143.0  0.99420  3.05   \n",
       "587       0.034                 37.0                 136.0  0.99200   NaN   \n",
       "621       0.083                 50.0                 171.0  0.99650  2.85   \n",
       "697       0.039                 62.0                 204.0  0.99820   NaN   \n",
       "747         NaN                 56.0                 177.0  0.99280  3.04   \n",
       "812       0.055                  9.0                 160.0  0.99405  3.42   \n",
       "909       0.049                 46.0                 217.0  0.99850  3.08   \n",
       "972       0.039                 38.0                 153.0  0.99940   NaN   \n",
       "1079      0.046                 29.0                 227.0  0.99520  3.29   \n",
       "2894      0.044                 22.0                  68.0  0.99014  3.15   \n",
       "2902      0.036                 38.0                 155.0  0.99622  3.27   \n",
       "4892      0.032                 29.0                 112.0  0.99298   NaN   \n",
       "4895      0.041                 30.0                 111.0  0.99254  2.99   \n",
       "6320      0.079                 39.0                  55.0  0.99560  3.39   \n",
       "6321      0.123                 14.0                  31.0  0.99680  3.50   \n",
       "6428      0.063                  9.0                  18.0  0.99444   NaN   \n",
       "6429      0.081                 13.0                  28.0  0.99631   NaN   \n",
       "6486      0.068                 34.0                 102.0  0.99414  3.27   \n",
       "6493      0.062                 39.0                  51.0  0.99512  3.52   \n",
       "\n",
       "      sulphates  alcohol  quality  \n",
       "17         0.39     12.8        1  \n",
       "33         0.51      9.0        0  \n",
       "54         0.38      9.1        0  \n",
       "86         0.44      8.7        0  \n",
       "98         0.30     10.1        0  \n",
       "139        0.52     11.8        0  \n",
       "174        0.64     10.2        0  \n",
       "224         NaN     10.4        0  \n",
       "249        0.45     10.0        0  \n",
       "267        0.57      9.7        0  \n",
       "268        0.57      9.7        0  \n",
       "368        0.64     10.6        0  \n",
       "438        0.61      9.9        1  \n",
       "440        0.59     11.6        1  \n",
       "518        0.64     10.8        0  \n",
       "521        0.74      9.8        0  \n",
       "587        0.50     10.8        1  \n",
       "621        0.50      9.0        0  \n",
       "697        0.60      9.5        0  \n",
       "747        0.32     11.5        0  \n",
       "812        0.50      9.1        1  \n",
       "909        0.53      8.8        0  \n",
       "972        0.42      9.0        0  \n",
       "1079       0.53     10.1        0  \n",
       "2894       0.50     11.7        1  \n",
       "2902        NaN      9.4        0  \n",
       "4892       0.54      9.7        0  \n",
       "4895       0.46      9.4        0  \n",
       "6320       0.84     11.4        0  \n",
       "6321        NaN     11.0        0  \n",
       "6428       0.69     11.3        0  \n",
       "6429       0.66     10.2        0  \n",
       "6486       0.78     12.8        0  \n",
       "6493        NaN     11.2        0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for missing values. we view all the rows that contain the missing values to determine their impact on the dataset.\n",
    "dataset[pd.isnull(dataset).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62afae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there only a few rows i decided to drop the rows as they won't have an impact on the model. \n",
    "dataset= dataset.dropna()\n",
    "dataset= dataset.drop(labels=['type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed29771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol, quality]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking again if there are still missing values.\n",
    "dataset[pd.isnull(dataset).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d47aa244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4290\n",
      "1    4290\n",
      "Name: quality, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separating features and target.\n",
    "X = dataset.drop('quality', axis=1)\n",
    "y = dataset['quality']\n",
    "\n",
    "# Setting SMOTE and transforming the data.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Checking for the balance of the classes again.\n",
    "print(y_smote.value_counts())\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaf183c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will check which Scaler performs the best.\n",
    "feature_scaler = StandardScaler()\n",
    "#feature_scaler = Normalizer()\n",
    "#feature_scaler=MaxAbsScaler()\n",
    "#feature_scaler=MinMaxScaler()\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "X_test = feature_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "787ba83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64277740\n",
      "Iteration 2, loss = 0.52387192\n",
      "Iteration 3, loss = 0.48926543\n",
      "Iteration 4, loss = 0.48097936\n",
      "Iteration 5, loss = 0.47780397\n",
      "Iteration 6, loss = 0.47625704\n",
      "Iteration 7, loss = 0.47465413\n",
      "Iteration 8, loss = 0.47298249\n",
      "Iteration 9, loss = 0.47187555\n",
      "Iteration 10, loss = 0.46954765\n",
      "Iteration 11, loss = 0.46836316\n",
      "Iteration 12, loss = 0.46731720\n",
      "Iteration 13, loss = 0.46700955\n",
      "Iteration 14, loss = 0.46550143\n",
      "Iteration 15, loss = 0.46413484\n",
      "Iteration 16, loss = 0.46321313\n",
      "Iteration 17, loss = 0.46163459\n",
      "Iteration 18, loss = 0.46136046\n",
      "Iteration 19, loss = 0.45959447\n",
      "Iteration 20, loss = 0.45875966\n",
      "Iteration 21, loss = 0.45742984\n",
      "Iteration 22, loss = 0.45676428\n",
      "Iteration 23, loss = 0.45525810\n",
      "Iteration 24, loss = 0.45486071\n",
      "Iteration 25, loss = 0.45383431\n",
      "Iteration 26, loss = 0.45275811\n",
      "Iteration 27, loss = 0.45252174\n",
      "Iteration 28, loss = 0.45176116\n",
      "Iteration 29, loss = 0.45082573\n",
      "Iteration 30, loss = 0.44980247\n",
      "Iteration 31, loss = 0.44975115\n",
      "Iteration 32, loss = 0.44919586\n",
      "Iteration 33, loss = 0.44834957\n",
      "Iteration 34, loss = 0.44745060\n",
      "Iteration 35, loss = 0.44717552\n",
      "Iteration 36, loss = 0.44671481\n",
      "Iteration 37, loss = 0.44612015\n",
      "Iteration 38, loss = 0.44573829\n",
      "Iteration 39, loss = 0.44534276\n",
      "Iteration 40, loss = 0.44506813\n",
      "Iteration 41, loss = 0.44485382\n",
      "Iteration 42, loss = 0.44426845\n",
      "Iteration 43, loss = 0.44420102\n",
      "Iteration 44, loss = 0.44363894\n",
      "Iteration 45, loss = 0.44387397\n",
      "Iteration 46, loss = 0.44278586\n",
      "Iteration 47, loss = 0.44271781\n",
      "Iteration 48, loss = 0.44210973\n",
      "Iteration 49, loss = 0.44160387\n",
      "Iteration 50, loss = 0.44125546\n",
      "Iteration 51, loss = 0.44158905\n",
      "Iteration 52, loss = 0.44116853\n",
      "Iteration 53, loss = 0.44051155\n",
      "Iteration 54, loss = 0.44031766\n",
      "Iteration 55, loss = 0.43994631\n",
      "Iteration 56, loss = 0.43987840\n",
      "Iteration 57, loss = 0.43997616\n",
      "Iteration 58, loss = 0.43917175\n",
      "Iteration 59, loss = 0.43925064\n",
      "Iteration 60, loss = 0.43892532\n",
      "Iteration 61, loss = 0.43863075\n",
      "Iteration 62, loss = 0.43906682\n",
      "Iteration 63, loss = 0.43772782\n",
      "Iteration 64, loss = 0.43833462\n",
      "Iteration 65, loss = 0.43879691\n",
      "Iteration 66, loss = 0.43839958\n",
      "Iteration 67, loss = 0.43771276\n",
      "Iteration 68, loss = 0.43731400\n",
      "Iteration 69, loss = 0.43786989\n",
      "Iteration 70, loss = 0.43700430\n",
      "Iteration 71, loss = 0.43613943\n",
      "Iteration 72, loss = 0.43638144\n",
      "Iteration 73, loss = 0.43534250\n",
      "Iteration 74, loss = 0.43569548\n",
      "Iteration 75, loss = 0.43557839\n",
      "Iteration 76, loss = 0.43486072\n",
      "Iteration 77, loss = 0.43481653\n",
      "Iteration 78, loss = 0.43493066\n",
      "Iteration 79, loss = 0.43390711\n",
      "Iteration 80, loss = 0.43366983\n",
      "Iteration 81, loss = 0.43345075\n",
      "Iteration 82, loss = 0.43351394\n",
      "Iteration 83, loss = 0.43366129\n",
      "Iteration 84, loss = 0.43311627\n",
      "Iteration 85, loss = 0.43318027\n",
      "Iteration 86, loss = 0.43249741\n",
      "Iteration 87, loss = 0.43372221\n",
      "Iteration 88, loss = 0.43247098\n",
      "Iteration 89, loss = 0.43189387\n",
      "Iteration 90, loss = 0.43265919\n",
      "Iteration 91, loss = 0.43186744\n",
      "Iteration 92, loss = 0.43174877\n",
      "Iteration 93, loss = 0.43150738\n",
      "Iteration 94, loss = 0.43156221\n",
      "Iteration 95, loss = 0.43150809\n",
      "Iteration 96, loss = 0.43184704\n",
      "Iteration 97, loss = 0.43136035\n",
      "Iteration 98, loss = 0.43113044\n",
      "Iteration 99, loss = 0.43086378\n",
      "Iteration 100, loss = 0.43086134\n",
      "Iteration 101, loss = 0.43113268\n",
      "Iteration 102, loss = 0.43023525\n",
      "Iteration 103, loss = 0.43126820\n",
      "Iteration 104, loss = 0.43111549\n",
      "Iteration 105, loss = 0.43078663\n",
      "Iteration 106, loss = 0.43065151\n",
      "Iteration 107, loss = 0.43029368\n",
      "Iteration 108, loss = 0.43001574\n",
      "Iteration 109, loss = 0.43005227\n",
      "Iteration 110, loss = 0.43014845\n",
      "Iteration 111, loss = 0.43003512\n",
      "Iteration 112, loss = 0.42985112\n",
      "Iteration 113, loss = 0.42965110\n",
      "Iteration 114, loss = 0.43018153\n",
      "Iteration 115, loss = 0.42961232\n",
      "Iteration 116, loss = 0.42972352\n",
      "Iteration 117, loss = 0.42990191\n",
      "Iteration 118, loss = 0.42973151\n",
      "Iteration 119, loss = 0.42916493\n",
      "Iteration 120, loss = 0.42968742\n",
      "Iteration 121, loss = 0.42961648\n",
      "Iteration 122, loss = 0.42907202\n",
      "Iteration 123, loss = 0.42927064\n",
      "Iteration 124, loss = 0.42980291\n",
      "Iteration 125, loss = 0.42928809\n",
      "Iteration 126, loss = 0.42842400\n",
      "Iteration 127, loss = 0.42894068\n",
      "Iteration 128, loss = 0.42919939\n",
      "Iteration 129, loss = 0.42937046\n",
      "Iteration 130, loss = 0.42921381\n",
      "Iteration 131, loss = 0.42860263\n",
      "Iteration 132, loss = 0.42813322\n",
      "Iteration 133, loss = 0.42876630\n",
      "Iteration 134, loss = 0.42861379\n",
      "Iteration 135, loss = 0.42859606\n",
      "Iteration 136, loss = 0.42840838\n",
      "Iteration 137, loss = 0.42775458\n",
      "Iteration 138, loss = 0.42815118\n",
      "Iteration 139, loss = 0.42806087\n",
      "Iteration 140, loss = 0.42787952\n",
      "Iteration 141, loss = 0.42797881\n",
      "Iteration 142, loss = 0.42818276\n",
      "Iteration 143, loss = 0.42812235\n",
      "Iteration 144, loss = 0.42824426\n",
      "Iteration 145, loss = 0.42823148\n",
      "Iteration 146, loss = 0.42787656\n",
      "Iteration 147, loss = 0.42837403\n",
      "Iteration 148, loss = 0.42808441\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60831716\n",
      "Iteration 2, loss = 0.49804099\n",
      "Iteration 3, loss = 0.48326501\n",
      "Iteration 4, loss = 0.48009501\n",
      "Iteration 5, loss = 0.47883340\n",
      "Iteration 6, loss = 0.47772017\n",
      "Iteration 7, loss = 0.47554579\n",
      "Iteration 8, loss = 0.47384858\n",
      "Iteration 9, loss = 0.47303983\n",
      "Iteration 10, loss = 0.47177985\n",
      "Iteration 11, loss = 0.47111651\n",
      "Iteration 12, loss = 0.46986848\n",
      "Iteration 13, loss = 0.46839370\n",
      "Iteration 14, loss = 0.46739849\n",
      "Iteration 15, loss = 0.46605916\n",
      "Iteration 16, loss = 0.46510331\n",
      "Iteration 17, loss = 0.46477304\n",
      "Iteration 18, loss = 0.46240839\n",
      "Iteration 19, loss = 0.46162407\n",
      "Iteration 20, loss = 0.45943592\n",
      "Iteration 21, loss = 0.45914393\n",
      "Iteration 22, loss = 0.45788319\n",
      "Iteration 23, loss = 0.45632405\n",
      "Iteration 24, loss = 0.45505197\n",
      "Iteration 25, loss = 0.45367730\n",
      "Iteration 26, loss = 0.45253475\n",
      "Iteration 27, loss = 0.45128027\n",
      "Iteration 28, loss = 0.45001865\n",
      "Iteration 29, loss = 0.44891136\n",
      "Iteration 30, loss = 0.44722575\n",
      "Iteration 31, loss = 0.44688828\n",
      "Iteration 32, loss = 0.44565132\n",
      "Iteration 33, loss = 0.44372599\n",
      "Iteration 34, loss = 0.44269528\n",
      "Iteration 35, loss = 0.44195341\n",
      "Iteration 36, loss = 0.44042085\n",
      "Iteration 37, loss = 0.43995173\n",
      "Iteration 38, loss = 0.43813801\n",
      "Iteration 39, loss = 0.43893795\n",
      "Iteration 40, loss = 0.43616791\n",
      "Iteration 41, loss = 0.43471119\n",
      "Iteration 42, loss = 0.43522211\n",
      "Iteration 43, loss = 0.43344087\n",
      "Iteration 44, loss = 0.43224315\n",
      "Iteration 45, loss = 0.43202208\n",
      "Iteration 46, loss = 0.43006501\n",
      "Iteration 47, loss = 0.42941087\n",
      "Iteration 48, loss = 0.42821648\n",
      "Iteration 49, loss = 0.42761431\n",
      "Iteration 50, loss = 0.42659750\n",
      "Iteration 51, loss = 0.42532125\n",
      "Iteration 52, loss = 0.42481207\n",
      "Iteration 53, loss = 0.42415796\n",
      "Iteration 54, loss = 0.42306442\n",
      "Iteration 55, loss = 0.42221072\n",
      "Iteration 56, loss = 0.42164679\n",
      "Iteration 57, loss = 0.42117341\n",
      "Iteration 58, loss = 0.42089993\n",
      "Iteration 59, loss = 0.41932045\n",
      "Iteration 60, loss = 0.41843690\n",
      "Iteration 61, loss = 0.41803804\n",
      "Iteration 62, loss = 0.41725291\n",
      "Iteration 63, loss = 0.41637138\n",
      "Iteration 64, loss = 0.41667544\n",
      "Iteration 65, loss = 0.41609672\n",
      "Iteration 66, loss = 0.41469035\n",
      "Iteration 67, loss = 0.41441030\n",
      "Iteration 68, loss = 0.41476838\n",
      "Iteration 69, loss = 0.41378719\n",
      "Iteration 70, loss = 0.41243773\n",
      "Iteration 71, loss = 0.41235924\n",
      "Iteration 72, loss = 0.41176511\n",
      "Iteration 73, loss = 0.41168262\n",
      "Iteration 74, loss = 0.41096334\n",
      "Iteration 75, loss = 0.41075474\n",
      "Iteration 76, loss = 0.41090839\n",
      "Iteration 77, loss = 0.41080841\n",
      "Iteration 78, loss = 0.40916561\n",
      "Iteration 79, loss = 0.41043375\n",
      "Iteration 80, loss = 0.40941256\n",
      "Iteration 81, loss = 0.40866245\n",
      "Iteration 82, loss = 0.40907028\n",
      "Iteration 83, loss = 0.40824194\n",
      "Iteration 84, loss = 0.40885918\n",
      "Iteration 85, loss = 0.40828894\n",
      "Iteration 86, loss = 0.40705055\n",
      "Iteration 87, loss = 0.40693823\n",
      "Iteration 88, loss = 0.40732701\n",
      "Iteration 89, loss = 0.40656198\n",
      "Iteration 90, loss = 0.40641831\n",
      "Iteration 91, loss = 0.40644089\n",
      "Iteration 92, loss = 0.40611792\n",
      "Iteration 93, loss = 0.40616153\n",
      "Iteration 94, loss = 0.40578597\n",
      "Iteration 95, loss = 0.40540860\n",
      "Iteration 96, loss = 0.40598987\n",
      "Iteration 97, loss = 0.40434098\n",
      "Iteration 98, loss = 0.40536484\n",
      "Iteration 99, loss = 0.40434036\n",
      "Iteration 100, loss = 0.40488827\n",
      "Iteration 101, loss = 0.40419768\n",
      "Iteration 102, loss = 0.40383033\n",
      "Iteration 103, loss = 0.40363964\n",
      "Iteration 104, loss = 0.40316415\n",
      "Iteration 105, loss = 0.40300340\n",
      "Iteration 106, loss = 0.40196991\n",
      "Iteration 107, loss = 0.40209599\n",
      "Iteration 108, loss = 0.40160710\n",
      "Iteration 109, loss = 0.40216621\n",
      "Iteration 110, loss = 0.40198145\n",
      "Iteration 111, loss = 0.40119970\n",
      "Iteration 112, loss = 0.40125631\n",
      "Iteration 113, loss = 0.40110280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 114, loss = 0.40147116\n",
      "Iteration 115, loss = 0.40054877\n",
      "Iteration 116, loss = 0.40008179\n",
      "Iteration 117, loss = 0.40128038\n",
      "Iteration 118, loss = 0.40182221\n",
      "Iteration 119, loss = 0.39994957\n",
      "Iteration 120, loss = 0.40008068\n",
      "Iteration 121, loss = 0.39975822\n",
      "Iteration 122, loss = 0.40002512\n",
      "Iteration 123, loss = 0.39922513\n",
      "Iteration 124, loss = 0.39851104\n",
      "Iteration 125, loss = 0.39833532\n",
      "Iteration 126, loss = 0.39848211\n",
      "Iteration 127, loss = 0.39988032\n",
      "Iteration 128, loss = 0.39836251\n",
      "Iteration 129, loss = 0.39845774\n",
      "Iteration 130, loss = 0.39777561\n",
      "Iteration 131, loss = 0.39752869\n",
      "Iteration 132, loss = 0.39745947\n",
      "Iteration 133, loss = 0.39702845\n",
      "Iteration 134, loss = 0.39736509\n",
      "Iteration 135, loss = 0.39767830\n",
      "Iteration 136, loss = 0.39637888\n",
      "Iteration 137, loss = 0.39669993\n",
      "Iteration 138, loss = 0.39680463\n",
      "Iteration 139, loss = 0.39754633\n",
      "Iteration 140, loss = 0.39589109\n",
      "Iteration 141, loss = 0.39632095\n",
      "Iteration 142, loss = 0.39637376\n",
      "Iteration 143, loss = 0.39624036\n",
      "Iteration 144, loss = 0.39515351\n",
      "Iteration 145, loss = 0.39727733\n",
      "Iteration 146, loss = 0.39578111\n",
      "Iteration 147, loss = 0.39469216\n",
      "Iteration 148, loss = 0.39507013\n",
      "Iteration 149, loss = 0.39547323\n",
      "Iteration 150, loss = 0.39566309\n",
      "Iteration 151, loss = 0.39432001\n",
      "Iteration 152, loss = 0.39449246\n",
      "Iteration 153, loss = 0.39520588\n",
      "Iteration 154, loss = 0.39450744\n",
      "Iteration 155, loss = 0.39375111\n",
      "Iteration 156, loss = 0.39376839\n",
      "Iteration 157, loss = 0.39295460\n",
      "Iteration 158, loss = 0.39372832\n",
      "Iteration 159, loss = 0.39411357\n",
      "Iteration 160, loss = 0.39412947\n",
      "Iteration 161, loss = 0.39304590\n",
      "Iteration 162, loss = 0.39306981\n",
      "Iteration 163, loss = 0.39333052\n",
      "Iteration 164, loss = 0.39283215\n",
      "Iteration 165, loss = 0.39239485\n",
      "Iteration 166, loss = 0.39255508\n",
      "Iteration 167, loss = 0.39205949\n",
      "Iteration 168, loss = 0.39264339\n",
      "Iteration 169, loss = 0.39193921\n",
      "Iteration 170, loss = 0.39241834\n",
      "Iteration 171, loss = 0.39281994\n",
      "Iteration 172, loss = 0.39159778\n",
      "Iteration 173, loss = 0.39141938\n",
      "Iteration 174, loss = 0.39176208\n",
      "Iteration 175, loss = 0.39136433\n",
      "Iteration 176, loss = 0.39211468\n",
      "Iteration 177, loss = 0.39150345\n",
      "Iteration 178, loss = 0.39131037\n",
      "Iteration 179, loss = 0.39069480\n",
      "Iteration 180, loss = 0.39080497\n",
      "Iteration 181, loss = 0.39090722\n",
      "Iteration 182, loss = 0.39076335\n",
      "Iteration 183, loss = 0.39076453\n",
      "Iteration 184, loss = 0.39066916\n",
      "Iteration 185, loss = 0.39017606\n",
      "Iteration 186, loss = 0.39081125\n",
      "Iteration 187, loss = 0.39090396\n",
      "Iteration 188, loss = 0.39134881\n",
      "Iteration 189, loss = 0.39095717\n",
      "Iteration 190, loss = 0.39016348\n",
      "Iteration 191, loss = 0.39001041\n",
      "Iteration 192, loss = 0.39019442\n",
      "Iteration 193, loss = 0.39124446\n",
      "Iteration 194, loss = 0.39158702\n",
      "Iteration 195, loss = 0.38970851\n",
      "Iteration 196, loss = 0.39150667\n",
      "Iteration 197, loss = 0.39007081\n",
      "Iteration 198, loss = 0.38962431\n",
      "Iteration 199, loss = 0.38996966\n",
      "Iteration 200, loss = 0.39012410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60285996\n",
      "Iteration 2, loss = 0.49513051\n",
      "Iteration 3, loss = 0.48310718\n",
      "Iteration 4, loss = 0.48092412\n",
      "Iteration 5, loss = 0.47980094\n",
      "Iteration 6, loss = 0.47928300\n",
      "Iteration 7, loss = 0.47795884\n",
      "Iteration 8, loss = 0.47714060\n",
      "Iteration 9, loss = 0.47582839\n",
      "Iteration 10, loss = 0.47561712\n",
      "Iteration 11, loss = 0.47335088\n",
      "Iteration 12, loss = 0.47193180\n",
      "Iteration 13, loss = 0.47114980\n",
      "Iteration 14, loss = 0.46996091\n",
      "Iteration 15, loss = 0.46840895\n",
      "Iteration 16, loss = 0.46646670\n",
      "Iteration 17, loss = 0.46516935\n",
      "Iteration 18, loss = 0.46289784\n",
      "Iteration 19, loss = 0.46143788\n",
      "Iteration 20, loss = 0.46039244\n",
      "Iteration 21, loss = 0.45917660\n",
      "Iteration 22, loss = 0.45748372\n",
      "Iteration 23, loss = 0.45624521\n",
      "Iteration 24, loss = 0.45399723\n",
      "Iteration 25, loss = 0.45296758\n",
      "Iteration 26, loss = 0.45172183\n",
      "Iteration 27, loss = 0.44991947\n",
      "Iteration 28, loss = 0.44830220\n",
      "Iteration 29, loss = 0.44749233\n",
      "Iteration 30, loss = 0.44602940\n",
      "Iteration 31, loss = 0.44482914\n",
      "Iteration 32, loss = 0.44375296\n",
      "Iteration 33, loss = 0.44237082\n",
      "Iteration 34, loss = 0.44199405\n",
      "Iteration 35, loss = 0.44061456\n",
      "Iteration 36, loss = 0.43905164\n",
      "Iteration 37, loss = 0.43771271\n",
      "Iteration 38, loss = 0.43762141\n",
      "Iteration 39, loss = 0.43733028\n",
      "Iteration 40, loss = 0.43463536\n",
      "Iteration 41, loss = 0.43478550\n",
      "Iteration 42, loss = 0.43363385\n",
      "Iteration 43, loss = 0.43281312\n",
      "Iteration 44, loss = 0.43219730\n",
      "Iteration 45, loss = 0.43207204\n",
      "Iteration 46, loss = 0.43024795\n",
      "Iteration 47, loss = 0.42999382\n",
      "Iteration 48, loss = 0.42836476\n",
      "Iteration 49, loss = 0.42808733\n",
      "Iteration 50, loss = 0.42696754\n",
      "Iteration 51, loss = 0.42517737\n",
      "Iteration 52, loss = 0.42594756\n",
      "Iteration 53, loss = 0.42496677\n",
      "Iteration 54, loss = 0.42405388\n",
      "Iteration 55, loss = 0.42394834\n",
      "Iteration 56, loss = 0.42205130\n",
      "Iteration 57, loss = 0.42155869\n",
      "Iteration 58, loss = 0.42079192\n",
      "Iteration 59, loss = 0.42050936\n",
      "Iteration 60, loss = 0.41922724\n",
      "Iteration 61, loss = 0.41813211\n",
      "Iteration 62, loss = 0.41863087\n",
      "Iteration 63, loss = 0.41703728\n",
      "Iteration 64, loss = 0.41615401\n",
      "Iteration 65, loss = 0.41514267\n",
      "Iteration 66, loss = 0.41430270\n",
      "Iteration 67, loss = 0.41454726\n",
      "Iteration 68, loss = 0.41333692\n",
      "Iteration 69, loss = 0.41359498\n",
      "Iteration 70, loss = 0.41298892\n",
      "Iteration 71, loss = 0.41225709\n",
      "Iteration 72, loss = 0.41153180\n",
      "Iteration 73, loss = 0.41129411\n",
      "Iteration 74, loss = 0.40981300\n",
      "Iteration 75, loss = 0.41063125\n",
      "Iteration 76, loss = 0.40882257\n",
      "Iteration 77, loss = 0.40862230\n",
      "Iteration 78, loss = 0.40807972\n",
      "Iteration 79, loss = 0.40763395\n",
      "Iteration 80, loss = 0.40778261\n",
      "Iteration 81, loss = 0.40724895\n",
      "Iteration 82, loss = 0.40605806\n",
      "Iteration 83, loss = 0.40684555\n",
      "Iteration 84, loss = 0.40521566\n",
      "Iteration 85, loss = 0.40474018\n",
      "Iteration 86, loss = 0.40511087\n",
      "Iteration 87, loss = 0.40499518\n",
      "Iteration 88, loss = 0.40408272\n",
      "Iteration 89, loss = 0.40283677\n",
      "Iteration 90, loss = 0.40297873\n",
      "Iteration 91, loss = 0.40180632\n",
      "Iteration 92, loss = 0.40263970\n",
      "Iteration 93, loss = 0.40217791\n",
      "Iteration 94, loss = 0.40075507\n",
      "Iteration 95, loss = 0.40064130\n",
      "Iteration 96, loss = 0.40106518\n",
      "Iteration 97, loss = 0.40040491\n",
      "Iteration 98, loss = 0.39953943\n",
      "Iteration 99, loss = 0.39817334\n",
      "Iteration 100, loss = 0.39876010\n",
      "Iteration 101, loss = 0.39841198\n",
      "Iteration 102, loss = 0.39807035\n",
      "Iteration 103, loss = 0.39858757\n",
      "Iteration 104, loss = 0.39603646\n",
      "Iteration 105, loss = 0.39653159\n",
      "Iteration 106, loss = 0.39598294\n",
      "Iteration 107, loss = 0.39512035\n",
      "Iteration 108, loss = 0.39510529\n",
      "Iteration 109, loss = 0.39512165\n",
      "Iteration 110, loss = 0.39504946\n",
      "Iteration 111, loss = 0.39372174\n",
      "Iteration 112, loss = 0.39430785\n",
      "Iteration 113, loss = 0.39306643\n",
      "Iteration 114, loss = 0.39297096\n",
      "Iteration 115, loss = 0.39258460\n",
      "Iteration 116, loss = 0.39295220\n",
      "Iteration 117, loss = 0.39170511\n",
      "Iteration 118, loss = 0.39235251\n",
      "Iteration 119, loss = 0.39126404\n",
      "Iteration 120, loss = 0.39222649\n",
      "Iteration 121, loss = 0.39066322\n",
      "Iteration 122, loss = 0.38989829\n",
      "Iteration 123, loss = 0.38920893\n",
      "Iteration 124, loss = 0.38892185\n",
      "Iteration 125, loss = 0.38953857\n",
      "Iteration 126, loss = 0.38890049\n",
      "Iteration 127, loss = 0.38860099\n",
      "Iteration 128, loss = 0.38739694\n",
      "Iteration 129, loss = 0.38754856\n",
      "Iteration 130, loss = 0.38776740\n",
      "Iteration 131, loss = 0.38799765\n",
      "Iteration 132, loss = 0.38602410\n",
      "Iteration 133, loss = 0.38617524\n",
      "Iteration 134, loss = 0.38697973\n",
      "Iteration 135, loss = 0.38603563\n",
      "Iteration 136, loss = 0.38614637\n",
      "Iteration 137, loss = 0.38577671\n",
      "Iteration 138, loss = 0.38506558\n",
      "Iteration 139, loss = 0.38410946\n",
      "Iteration 140, loss = 0.38398994\n",
      "Iteration 141, loss = 0.38500790\n",
      "Iteration 142, loss = 0.38377560\n",
      "Iteration 143, loss = 0.38331294\n",
      "Iteration 144, loss = 0.38354014\n",
      "Iteration 145, loss = 0.38227533\n",
      "Iteration 146, loss = 0.38203495\n",
      "Iteration 147, loss = 0.38150446\n",
      "Iteration 148, loss = 0.38196773\n",
      "Iteration 149, loss = 0.38123971\n",
      "Iteration 150, loss = 0.38119384\n",
      "Iteration 151, loss = 0.38026204\n",
      "Iteration 152, loss = 0.38177931\n",
      "Iteration 153, loss = 0.38008301\n",
      "Iteration 154, loss = 0.38039086\n",
      "Iteration 155, loss = 0.38006331\n",
      "Iteration 156, loss = 0.37905412\n",
      "Iteration 157, loss = 0.37939507\n",
      "Iteration 158, loss = 0.37983062\n",
      "Iteration 159, loss = 0.37929642\n",
      "Iteration 160, loss = 0.38015271\n",
      "Iteration 161, loss = 0.37845100\n",
      "Iteration 162, loss = 0.37721828\n",
      "Iteration 163, loss = 0.37819960\n",
      "Iteration 164, loss = 0.37690173\n",
      "Iteration 165, loss = 0.37794104\n",
      "Iteration 166, loss = 0.37821617\n",
      "Iteration 167, loss = 0.37844140\n",
      "Iteration 168, loss = 0.37687280\n",
      "Iteration 169, loss = 0.37681134\n",
      "Iteration 170, loss = 0.37665068\n",
      "Iteration 171, loss = 0.37657154\n",
      "Iteration 172, loss = 0.37649274\n",
      "Iteration 173, loss = 0.37511490\n",
      "Iteration 174, loss = 0.37574660\n",
      "Iteration 175, loss = 0.37554726\n",
      "Iteration 176, loss = 0.37533755\n",
      "Iteration 177, loss = 0.37548835\n",
      "Iteration 178, loss = 0.37641889\n",
      "Iteration 179, loss = 0.37472588\n",
      "Iteration 180, loss = 0.37471490\n",
      "Iteration 181, loss = 0.37340564\n",
      "Iteration 182, loss = 0.37378619\n",
      "Iteration 183, loss = 0.37367437\n",
      "Iteration 184, loss = 0.37356067\n",
      "Iteration 185, loss = 0.37388284\n",
      "Iteration 186, loss = 0.37309768\n",
      "Iteration 187, loss = 0.37275171\n",
      "Iteration 188, loss = 0.37296605\n",
      "Iteration 189, loss = 0.37340181\n",
      "Iteration 190, loss = 0.37303323\n",
      "Iteration 191, loss = 0.37301312\n",
      "Iteration 192, loss = 0.37202218\n",
      "Iteration 193, loss = 0.37259728\n",
      "Iteration 194, loss = 0.37126310\n",
      "Iteration 195, loss = 0.37193749\n",
      "Iteration 196, loss = 0.37357433\n",
      "Iteration 197, loss = 0.37105227\n",
      "Iteration 198, loss = 0.37168200\n",
      "Iteration 199, loss = 0.37105355\n",
      "Iteration 200, loss = 0.37228307\n",
      "Iteration 1, loss = 0.62517252\n",
      "Iteration 2, loss = 0.49956985\n",
      "Iteration 3, loss = 0.48447200\n",
      "Iteration 4, loss = 0.48278077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.48131242\n",
      "Iteration 6, loss = 0.48082759\n",
      "Iteration 7, loss = 0.47907619\n",
      "Iteration 8, loss = 0.47791936\n",
      "Iteration 9, loss = 0.47653254\n",
      "Iteration 10, loss = 0.47579787\n",
      "Iteration 11, loss = 0.47359793\n",
      "Iteration 12, loss = 0.47250007\n",
      "Iteration 13, loss = 0.47125315\n",
      "Iteration 14, loss = 0.46896303\n",
      "Iteration 15, loss = 0.46759013\n",
      "Iteration 16, loss = 0.46508886\n",
      "Iteration 17, loss = 0.46362713\n",
      "Iteration 18, loss = 0.46181304\n",
      "Iteration 19, loss = 0.45958840\n",
      "Iteration 20, loss = 0.45772750\n",
      "Iteration 21, loss = 0.45517241\n",
      "Iteration 22, loss = 0.45283832\n",
      "Iteration 23, loss = 0.45235592\n",
      "Iteration 24, loss = 0.44949442\n",
      "Iteration 25, loss = 0.44732216\n",
      "Iteration 26, loss = 0.44616503\n",
      "Iteration 27, loss = 0.44393168\n",
      "Iteration 28, loss = 0.44190601\n",
      "Iteration 29, loss = 0.44080696\n",
      "Iteration 30, loss = 0.43793317\n",
      "Iteration 31, loss = 0.43869260\n",
      "Iteration 32, loss = 0.43647278\n",
      "Iteration 33, loss = 0.43465535\n",
      "Iteration 34, loss = 0.43286292\n",
      "Iteration 35, loss = 0.43230374\n",
      "Iteration 36, loss = 0.43124298\n",
      "Iteration 37, loss = 0.42983126\n",
      "Iteration 38, loss = 0.42814024\n",
      "Iteration 39, loss = 0.42767224\n",
      "Iteration 40, loss = 0.42713769\n",
      "Iteration 41, loss = 0.42487900\n",
      "Iteration 42, loss = 0.42451889\n",
      "Iteration 43, loss = 0.42520453\n",
      "Iteration 44, loss = 0.42325298\n",
      "Iteration 45, loss = 0.42177484\n",
      "Iteration 46, loss = 0.42159061\n",
      "Iteration 47, loss = 0.42070453\n",
      "Iteration 48, loss = 0.42053513\n",
      "Iteration 49, loss = 0.41910198\n",
      "Iteration 50, loss = 0.41933749\n",
      "Iteration 51, loss = 0.41788445\n",
      "Iteration 52, loss = 0.41761182\n",
      "Iteration 53, loss = 0.41717834\n",
      "Iteration 54, loss = 0.41706042\n",
      "Iteration 55, loss = 0.41623831\n",
      "Iteration 56, loss = 0.41572262\n",
      "Iteration 57, loss = 0.41623144\n",
      "Iteration 58, loss = 0.41459727\n",
      "Iteration 59, loss = 0.41358466\n",
      "Iteration 60, loss = 0.41460553\n",
      "Iteration 61, loss = 0.41288100\n",
      "Iteration 62, loss = 0.41346014\n",
      "Iteration 63, loss = 0.41211046\n",
      "Iteration 64, loss = 0.41145248\n",
      "Iteration 65, loss = 0.41171911\n",
      "Iteration 66, loss = 0.41035103\n",
      "Iteration 67, loss = 0.40917112\n",
      "Iteration 68, loss = 0.40926181\n",
      "Iteration 69, loss = 0.40856325\n",
      "Iteration 70, loss = 0.40797389\n",
      "Iteration 71, loss = 0.40683461\n",
      "Iteration 72, loss = 0.40716377\n",
      "Iteration 73, loss = 0.40588555\n",
      "Iteration 74, loss = 0.40578855\n",
      "Iteration 75, loss = 0.40519347\n",
      "Iteration 76, loss = 0.40415427\n",
      "Iteration 77, loss = 0.40431086\n",
      "Iteration 78, loss = 0.40307381\n",
      "Iteration 79, loss = 0.40326406\n",
      "Iteration 80, loss = 0.40231459\n",
      "Iteration 81, loss = 0.40134129\n",
      "Iteration 82, loss = 0.40218761\n",
      "Iteration 83, loss = 0.40109575\n",
      "Iteration 84, loss = 0.40036305\n",
      "Iteration 85, loss = 0.39883579\n",
      "Iteration 86, loss = 0.39879539\n",
      "Iteration 87, loss = 0.39832587\n",
      "Iteration 88, loss = 0.39802964\n",
      "Iteration 89, loss = 0.39773722\n",
      "Iteration 90, loss = 0.39676983\n",
      "Iteration 91, loss = 0.39609468\n",
      "Iteration 92, loss = 0.39768530\n",
      "Iteration 93, loss = 0.39505272\n",
      "Iteration 94, loss = 0.39571604\n",
      "Iteration 95, loss = 0.39491043\n",
      "Iteration 96, loss = 0.39466257\n",
      "Iteration 97, loss = 0.39617991\n",
      "Iteration 98, loss = 0.39347332\n",
      "Iteration 99, loss = 0.39343196\n",
      "Iteration 100, loss = 0.39292224\n",
      "Iteration 101, loss = 0.39257008\n",
      "Iteration 102, loss = 0.39078870\n",
      "Iteration 103, loss = 0.39178079\n",
      "Iteration 104, loss = 0.39094539\n",
      "Iteration 105, loss = 0.38955359\n",
      "Iteration 106, loss = 0.39009683\n",
      "Iteration 107, loss = 0.38923724\n",
      "Iteration 108, loss = 0.38831950\n",
      "Iteration 109, loss = 0.38759833\n",
      "Iteration 110, loss = 0.38723776\n",
      "Iteration 111, loss = 0.38673007\n",
      "Iteration 112, loss = 0.38703713\n",
      "Iteration 113, loss = 0.38669800\n",
      "Iteration 114, loss = 0.38608116\n",
      "Iteration 115, loss = 0.38629284\n",
      "Iteration 116, loss = 0.38579960\n",
      "Iteration 117, loss = 0.38433169\n",
      "Iteration 118, loss = 0.38446601\n",
      "Iteration 119, loss = 0.38392342\n",
      "Iteration 120, loss = 0.38485138\n",
      "Iteration 121, loss = 0.38228360\n",
      "Iteration 122, loss = 0.38318387\n",
      "Iteration 123, loss = 0.38291676\n",
      "Iteration 124, loss = 0.38150139\n",
      "Iteration 125, loss = 0.38101042\n",
      "Iteration 126, loss = 0.38010981\n",
      "Iteration 127, loss = 0.37972850\n",
      "Iteration 128, loss = 0.37886260\n",
      "Iteration 129, loss = 0.37911453\n",
      "Iteration 130, loss = 0.37843906\n",
      "Iteration 131, loss = 0.37724817\n",
      "Iteration 132, loss = 0.37769010\n",
      "Iteration 133, loss = 0.37721392\n",
      "Iteration 134, loss = 0.37680542\n",
      "Iteration 135, loss = 0.37698336\n",
      "Iteration 136, loss = 0.37618046\n",
      "Iteration 137, loss = 0.37580628\n",
      "Iteration 138, loss = 0.37578683\n",
      "Iteration 139, loss = 0.37448874\n",
      "Iteration 140, loss = 0.37503366\n",
      "Iteration 141, loss = 0.37392910\n",
      "Iteration 142, loss = 0.37370460\n",
      "Iteration 143, loss = 0.37476603\n",
      "Iteration 144, loss = 0.37382871\n",
      "Iteration 145, loss = 0.37313473\n",
      "Iteration 146, loss = 0.37167755\n",
      "Iteration 147, loss = 0.37301589\n",
      "Iteration 148, loss = 0.37182324\n",
      "Iteration 149, loss = 0.37221626\n",
      "Iteration 150, loss = 0.37043516\n",
      "Iteration 151, loss = 0.37106145\n",
      "Iteration 152, loss = 0.37087304\n",
      "Iteration 153, loss = 0.37241172\n",
      "Iteration 154, loss = 0.37032223\n",
      "Iteration 155, loss = 0.36932484\n",
      "Iteration 156, loss = 0.36882821\n",
      "Iteration 157, loss = 0.36845004\n",
      "Iteration 158, loss = 0.36831611\n",
      "Iteration 159, loss = 0.36972467\n",
      "Iteration 160, loss = 0.36857192\n",
      "Iteration 161, loss = 0.36865088\n",
      "Iteration 162, loss = 0.36750442\n",
      "Iteration 163, loss = 0.36750623\n",
      "Iteration 164, loss = 0.36751215\n",
      "Iteration 165, loss = 0.36733224\n",
      "Iteration 166, loss = 0.36699786\n",
      "Iteration 167, loss = 0.36662085\n",
      "Iteration 168, loss = 0.36685442\n",
      "Iteration 169, loss = 0.36569541\n",
      "Iteration 170, loss = 0.36562979\n",
      "Iteration 171, loss = 0.36483967\n",
      "Iteration 172, loss = 0.36473372\n",
      "Iteration 173, loss = 0.36427015\n",
      "Iteration 174, loss = 0.36462676\n",
      "Iteration 175, loss = 0.36425034\n",
      "Iteration 176, loss = 0.36492567\n",
      "Iteration 177, loss = 0.36481009\n",
      "Iteration 178, loss = 0.36414132\n",
      "Iteration 179, loss = 0.36417212\n",
      "Iteration 180, loss = 0.36347138\n",
      "Iteration 181, loss = 0.36397640\n",
      "Iteration 182, loss = 0.36160454\n",
      "Iteration 183, loss = 0.36223713\n",
      "Iteration 184, loss = 0.36174899\n",
      "Iteration 185, loss = 0.36142785\n",
      "Iteration 186, loss = 0.36161254\n",
      "Iteration 187, loss = 0.36202847\n",
      "Iteration 188, loss = 0.36173588\n",
      "Iteration 189, loss = 0.36058012\n",
      "Iteration 190, loss = 0.36149768\n",
      "Iteration 191, loss = 0.36022303\n",
      "Iteration 192, loss = 0.36037526\n",
      "Iteration 193, loss = 0.36002814\n",
      "Iteration 194, loss = 0.35967593\n",
      "Iteration 195, loss = 0.35919352\n",
      "Iteration 196, loss = 0.35968137\n",
      "Iteration 197, loss = 0.36099004\n",
      "Iteration 198, loss = 0.35901229\n",
      "Iteration 199, loss = 0.35871326\n",
      "Iteration 200, loss = 0.35846400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60857093\n",
      "Iteration 2, loss = 0.50022536\n",
      "Iteration 3, loss = 0.48586372\n",
      "Iteration 4, loss = 0.48353549\n",
      "Iteration 5, loss = 0.48298258\n",
      "Iteration 6, loss = 0.48324564\n",
      "Iteration 7, loss = 0.48297898\n",
      "Iteration 8, loss = 0.48134703\n",
      "Iteration 9, loss = 0.48138000\n",
      "Iteration 10, loss = 0.47921194\n",
      "Iteration 11, loss = 0.47868251\n",
      "Iteration 12, loss = 0.47829101\n",
      "Iteration 13, loss = 0.47786704\n",
      "Iteration 14, loss = 0.47696249\n",
      "Iteration 15, loss = 0.47457120\n",
      "Iteration 16, loss = 0.47428984\n",
      "Iteration 17, loss = 0.47350413\n",
      "Iteration 18, loss = 0.47210758\n",
      "Iteration 19, loss = 0.47206035\n",
      "Iteration 20, loss = 0.46996672\n",
      "Iteration 21, loss = 0.46852792\n",
      "Iteration 22, loss = 0.46849658\n",
      "Iteration 23, loss = 0.46668344\n",
      "Iteration 24, loss = 0.46763489\n",
      "Iteration 25, loss = 0.46457883\n",
      "Iteration 26, loss = 0.46402032\n",
      "Iteration 27, loss = 0.46202945\n",
      "Iteration 28, loss = 0.46077205\n",
      "Iteration 29, loss = 0.46047662\n",
      "Iteration 30, loss = 0.45897362\n",
      "Iteration 31, loss = 0.45736245\n",
      "Iteration 32, loss = 0.45567367\n",
      "Iteration 33, loss = 0.45571958\n",
      "Iteration 34, loss = 0.45354559\n",
      "Iteration 35, loss = 0.45404321\n",
      "Iteration 36, loss = 0.44976347\n",
      "Iteration 37, loss = 0.44903157\n",
      "Iteration 38, loss = 0.44736865\n",
      "Iteration 39, loss = 0.44601064\n",
      "Iteration 40, loss = 0.44359837\n",
      "Iteration 41, loss = 0.44166608\n",
      "Iteration 42, loss = 0.44141087\n",
      "Iteration 43, loss = 0.43816256\n",
      "Iteration 44, loss = 0.43793205\n",
      "Iteration 45, loss = 0.43638681\n",
      "Iteration 46, loss = 0.43508604\n",
      "Iteration 47, loss = 0.43387695\n",
      "Iteration 48, loss = 0.43328243\n",
      "Iteration 49, loss = 0.43090763\n",
      "Iteration 50, loss = 0.42881773\n",
      "Iteration 51, loss = 0.42758204\n",
      "Iteration 52, loss = 0.42664464\n",
      "Iteration 53, loss = 0.42561488\n",
      "Iteration 54, loss = 0.42583355\n",
      "Iteration 55, loss = 0.42406454\n",
      "Iteration 56, loss = 0.42293895\n",
      "Iteration 57, loss = 0.42297034\n",
      "Iteration 58, loss = 0.42226347\n",
      "Iteration 59, loss = 0.41962365\n",
      "Iteration 60, loss = 0.41870040\n",
      "Iteration 61, loss = 0.41889970\n",
      "Iteration 62, loss = 0.41944002\n",
      "Iteration 63, loss = 0.41725783\n",
      "Iteration 64, loss = 0.41641948\n",
      "Iteration 65, loss = 0.41520880\n",
      "Iteration 66, loss = 0.41554089\n",
      "Iteration 67, loss = 0.41500958\n",
      "Iteration 68, loss = 0.41424491\n",
      "Iteration 69, loss = 0.41364495\n",
      "Iteration 70, loss = 0.41236828\n",
      "Iteration 71, loss = 0.41200610\n",
      "Iteration 72, loss = 0.41129591\n",
      "Iteration 73, loss = 0.41161862\n",
      "Iteration 74, loss = 0.41048172\n",
      "Iteration 75, loss = 0.41297553\n",
      "Iteration 76, loss = 0.40977747\n",
      "Iteration 77, loss = 0.40992746\n",
      "Iteration 78, loss = 0.40860618\n",
      "Iteration 79, loss = 0.40852540\n",
      "Iteration 80, loss = 0.40854095\n",
      "Iteration 81, loss = 0.40772370\n",
      "Iteration 82, loss = 0.40735422\n",
      "Iteration 83, loss = 0.40615485\n",
      "Iteration 84, loss = 0.40527039\n",
      "Iteration 85, loss = 0.40475700\n",
      "Iteration 86, loss = 0.40501711\n",
      "Iteration 87, loss = 0.40449109\n",
      "Iteration 88, loss = 0.40380180\n",
      "Iteration 89, loss = 0.40487695\n",
      "Iteration 90, loss = 0.40372684\n",
      "Iteration 91, loss = 0.40290070\n",
      "Iteration 92, loss = 0.40263274\n",
      "Iteration 93, loss = 0.40192120\n",
      "Iteration 94, loss = 0.40232496\n",
      "Iteration 95, loss = 0.40061006\n",
      "Iteration 96, loss = 0.40109808\n",
      "Iteration 97, loss = 0.40055602\n",
      "Iteration 98, loss = 0.39994208\n",
      "Iteration 99, loss = 0.39964760\n",
      "Iteration 100, loss = 0.39911444\n",
      "Iteration 101, loss = 0.39724005\n",
      "Iteration 102, loss = 0.39813834\n",
      "Iteration 103, loss = 0.39612513\n",
      "Iteration 104, loss = 0.39707005\n",
      "Iteration 105, loss = 0.39626032\n",
      "Iteration 106, loss = 0.39710762\n",
      "Iteration 107, loss = 0.39464176\n",
      "Iteration 108, loss = 0.39327686\n",
      "Iteration 109, loss = 0.39812569\n",
      "Iteration 110, loss = 0.39379329\n",
      "Iteration 111, loss = 0.39208145\n",
      "Iteration 112, loss = 0.39311953\n",
      "Iteration 113, loss = 0.39269880\n",
      "Iteration 114, loss = 0.39132212\n",
      "Iteration 115, loss = 0.39136046\n",
      "Iteration 116, loss = 0.39087119\n",
      "Iteration 117, loss = 0.38965176\n",
      "Iteration 118, loss = 0.38926107\n",
      "Iteration 119, loss = 0.38859024\n",
      "Iteration 120, loss = 0.38863281\n",
      "Iteration 121, loss = 0.38793925\n",
      "Iteration 122, loss = 0.38747650\n",
      "Iteration 123, loss = 0.38710865\n",
      "Iteration 124, loss = 0.38720664\n",
      "Iteration 125, loss = 0.38657345\n",
      "Iteration 126, loss = 0.38574233\n",
      "Iteration 127, loss = 0.38591829\n",
      "Iteration 128, loss = 0.38525031\n",
      "Iteration 129, loss = 0.38591612\n",
      "Iteration 130, loss = 0.38530790\n",
      "Iteration 131, loss = 0.38384101\n",
      "Iteration 132, loss = 0.38469980\n",
      "Iteration 133, loss = 0.38397464\n",
      "Iteration 134, loss = 0.38343985\n",
      "Iteration 135, loss = 0.38322293\n",
      "Iteration 136, loss = 0.38099215\n",
      "Iteration 137, loss = 0.38140074\n",
      "Iteration 138, loss = 0.38020116\n",
      "Iteration 139, loss = 0.38124138\n",
      "Iteration 140, loss = 0.38089343\n",
      "Iteration 141, loss = 0.37912067\n",
      "Iteration 142, loss = 0.37959712\n",
      "Iteration 143, loss = 0.37946667\n",
      "Iteration 144, loss = 0.37945213\n",
      "Iteration 145, loss = 0.37786226\n",
      "Iteration 146, loss = 0.37832970\n",
      "Iteration 147, loss = 0.37701007\n",
      "Iteration 148, loss = 0.37716766\n",
      "Iteration 149, loss = 0.37600517\n",
      "Iteration 150, loss = 0.37793141\n",
      "Iteration 151, loss = 0.37667689\n",
      "Iteration 152, loss = 0.37529183\n",
      "Iteration 153, loss = 0.37460084\n",
      "Iteration 154, loss = 0.37445665\n",
      "Iteration 155, loss = 0.37465785\n",
      "Iteration 156, loss = 0.37603561\n",
      "Iteration 157, loss = 0.37293975\n",
      "Iteration 158, loss = 0.37421022\n",
      "Iteration 159, loss = 0.37357593\n",
      "Iteration 160, loss = 0.37242523\n",
      "Iteration 161, loss = 0.37172571\n",
      "Iteration 162, loss = 0.37172504\n",
      "Iteration 163, loss = 0.37138866\n",
      "Iteration 164, loss = 0.37122681\n",
      "Iteration 165, loss = 0.37097798\n",
      "Iteration 166, loss = 0.37196453\n",
      "Iteration 167, loss = 0.37056567\n",
      "Iteration 168, loss = 0.36980419\n",
      "Iteration 169, loss = 0.36915477\n",
      "Iteration 170, loss = 0.36942519\n",
      "Iteration 171, loss = 0.36846091\n",
      "Iteration 172, loss = 0.36853215\n",
      "Iteration 173, loss = 0.36795127\n",
      "Iteration 174, loss = 0.36825342\n",
      "Iteration 175, loss = 0.36686210\n",
      "Iteration 176, loss = 0.36749805\n",
      "Iteration 177, loss = 0.36763951\n",
      "Iteration 178, loss = 0.36620839\n",
      "Iteration 179, loss = 0.36520623\n",
      "Iteration 180, loss = 0.36584764\n",
      "Iteration 181, loss = 0.36608281\n",
      "Iteration 182, loss = 0.36534530\n",
      "Iteration 183, loss = 0.36499187\n",
      "Iteration 184, loss = 0.36405882\n",
      "Iteration 185, loss = 0.36531752\n",
      "Iteration 186, loss = 0.36382315\n",
      "Iteration 187, loss = 0.36461117\n",
      "Iteration 188, loss = 0.36417952\n",
      "Iteration 189, loss = 0.36272785\n",
      "Iteration 190, loss = 0.36195251\n",
      "Iteration 191, loss = 0.36359675\n",
      "Iteration 192, loss = 0.36182693\n",
      "Iteration 193, loss = 0.36232495\n",
      "Iteration 194, loss = 0.36112982\n",
      "Iteration 195, loss = 0.36150411\n",
      "Iteration 196, loss = 0.36127890\n",
      "Iteration 197, loss = 0.36189515\n",
      "Iteration 198, loss = 0.36110056\n",
      "Iteration 199, loss = 0.36048458\n",
      "Iteration 200, loss = 0.36080787\n",
      "Iteration 1, loss = 0.60313328\n",
      "Iteration 2, loss = 0.50404677\n",
      "Iteration 3, loss = 0.48805884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.48659842\n",
      "Iteration 5, loss = 0.48426366\n",
      "Iteration 6, loss = 0.48494069\n",
      "Iteration 7, loss = 0.48563956\n",
      "Iteration 8, loss = 0.48349245\n",
      "Iteration 9, loss = 0.48391950\n",
      "Iteration 10, loss = 0.48401962\n",
      "Iteration 11, loss = 0.48256879\n",
      "Iteration 12, loss = 0.48353684\n",
      "Iteration 13, loss = 0.48141276\n",
      "Iteration 14, loss = 0.48205646\n",
      "Iteration 15, loss = 0.48110082\n",
      "Iteration 16, loss = 0.48159302\n",
      "Iteration 17, loss = 0.47966155\n",
      "Iteration 18, loss = 0.47988524\n",
      "Iteration 19, loss = 0.47788341\n",
      "Iteration 20, loss = 0.47704755\n",
      "Iteration 21, loss = 0.47779755\n",
      "Iteration 22, loss = 0.47458725\n",
      "Iteration 23, loss = 0.47489556\n",
      "Iteration 24, loss = 0.47253687\n",
      "Iteration 25, loss = 0.47231569\n",
      "Iteration 26, loss = 0.47075653\n",
      "Iteration 27, loss = 0.46901946\n",
      "Iteration 28, loss = 0.46928755\n",
      "Iteration 29, loss = 0.46919009\n",
      "Iteration 30, loss = 0.46556789\n",
      "Iteration 31, loss = 0.46378477\n",
      "Iteration 32, loss = 0.46333935\n",
      "Iteration 33, loss = 0.46251301\n",
      "Iteration 34, loss = 0.45972103\n",
      "Iteration 35, loss = 0.45945184\n",
      "Iteration 36, loss = 0.45626344\n",
      "Iteration 37, loss = 0.45502638\n",
      "Iteration 38, loss = 0.45221222\n",
      "Iteration 39, loss = 0.45126235\n",
      "Iteration 40, loss = 0.45394931\n",
      "Iteration 41, loss = 0.44835238\n",
      "Iteration 42, loss = 0.44768876\n",
      "Iteration 43, loss = 0.44572698\n",
      "Iteration 44, loss = 0.44257380\n",
      "Iteration 45, loss = 0.44309591\n",
      "Iteration 46, loss = 0.44127991\n",
      "Iteration 47, loss = 0.44117455\n",
      "Iteration 48, loss = 0.44008574\n",
      "Iteration 49, loss = 0.44102232\n",
      "Iteration 50, loss = 0.43711824\n",
      "Iteration 51, loss = 0.43538474\n",
      "Iteration 52, loss = 0.43499773\n",
      "Iteration 53, loss = 0.43360138\n",
      "Iteration 54, loss = 0.43341383\n",
      "Iteration 55, loss = 0.43079615\n",
      "Iteration 56, loss = 0.43019761\n",
      "Iteration 57, loss = 0.42919425\n",
      "Iteration 58, loss = 0.42750295\n",
      "Iteration 59, loss = 0.42691218\n",
      "Iteration 60, loss = 0.42708713\n",
      "Iteration 61, loss = 0.42457249\n",
      "Iteration 62, loss = 0.42286307\n",
      "Iteration 63, loss = 0.42262819\n",
      "Iteration 64, loss = 0.42273058\n",
      "Iteration 65, loss = 0.42158268\n",
      "Iteration 66, loss = 0.42169314\n",
      "Iteration 67, loss = 0.41970215\n",
      "Iteration 68, loss = 0.41980796\n",
      "Iteration 69, loss = 0.41705789\n",
      "Iteration 70, loss = 0.41897185\n",
      "Iteration 71, loss = 0.41614693\n",
      "Iteration 72, loss = 0.41667085\n",
      "Iteration 73, loss = 0.41486727\n",
      "Iteration 74, loss = 0.41399806\n",
      "Iteration 75, loss = 0.41604083\n",
      "Iteration 76, loss = 0.41597183\n",
      "Iteration 77, loss = 0.41328752\n",
      "Iteration 78, loss = 0.41163273\n",
      "Iteration 79, loss = 0.41187686\n",
      "Iteration 80, loss = 0.41253776\n",
      "Iteration 81, loss = 0.41116211\n",
      "Iteration 82, loss = 0.41159434\n",
      "Iteration 83, loss = 0.40930901\n",
      "Iteration 84, loss = 0.41082171\n",
      "Iteration 85, loss = 0.40953499\n",
      "Iteration 86, loss = 0.40907113\n",
      "Iteration 87, loss = 0.40704397\n",
      "Iteration 88, loss = 0.40727481\n",
      "Iteration 89, loss = 0.40752208\n",
      "Iteration 90, loss = 0.40757890\n",
      "Iteration 91, loss = 0.40506273\n",
      "Iteration 92, loss = 0.40782315\n",
      "Iteration 93, loss = 0.40619106\n",
      "Iteration 94, loss = 0.40359538\n",
      "Iteration 95, loss = 0.40537868\n",
      "Iteration 96, loss = 0.40395604\n",
      "Iteration 97, loss = 0.40344725\n",
      "Iteration 98, loss = 0.40201875\n",
      "Iteration 99, loss = 0.40184436\n",
      "Iteration 100, loss = 0.40080783\n",
      "Iteration 101, loss = 0.40361153\n",
      "Iteration 102, loss = 0.40196808\n",
      "Iteration 103, loss = 0.39964808\n",
      "Iteration 104, loss = 0.40004886\n",
      "Iteration 105, loss = 0.40017813\n",
      "Iteration 106, loss = 0.39782762\n",
      "Iteration 107, loss = 0.39924927\n",
      "Iteration 108, loss = 0.39776423\n",
      "Iteration 109, loss = 0.39772482\n",
      "Iteration 110, loss = 0.39624089\n",
      "Iteration 111, loss = 0.39699500\n",
      "Iteration 112, loss = 0.39702360\n",
      "Iteration 113, loss = 0.39852694\n",
      "Iteration 114, loss = 0.39565778\n",
      "Iteration 115, loss = 0.39584456\n",
      "Iteration 116, loss = 0.39565658\n",
      "Iteration 117, loss = 0.39525380\n",
      "Iteration 118, loss = 0.39389407\n",
      "Iteration 119, loss = 0.39458876\n",
      "Iteration 120, loss = 0.39239854\n",
      "Iteration 121, loss = 0.39421812\n",
      "Iteration 122, loss = 0.39189668\n",
      "Iteration 123, loss = 0.39200027\n",
      "Iteration 124, loss = 0.39222174\n",
      "Iteration 125, loss = 0.39030135\n",
      "Iteration 126, loss = 0.39024539\n",
      "Iteration 127, loss = 0.39021182\n",
      "Iteration 128, loss = 0.39034419\n",
      "Iteration 129, loss = 0.38968445\n",
      "Iteration 130, loss = 0.38946370\n",
      "Iteration 131, loss = 0.38960697\n",
      "Iteration 132, loss = 0.38842140\n",
      "Iteration 133, loss = 0.38849278\n",
      "Iteration 134, loss = 0.38819492\n",
      "Iteration 135, loss = 0.38829785\n",
      "Iteration 136, loss = 0.38733798\n",
      "Iteration 137, loss = 0.38715626\n",
      "Iteration 138, loss = 0.38768574\n",
      "Iteration 139, loss = 0.38629942\n",
      "Iteration 140, loss = 0.38822872\n",
      "Iteration 141, loss = 0.38566401\n",
      "Iteration 142, loss = 0.38772848\n",
      "Iteration 143, loss = 0.38583809\n",
      "Iteration 144, loss = 0.38561138\n",
      "Iteration 145, loss = 0.38407039\n",
      "Iteration 146, loss = 0.38562790\n",
      "Iteration 147, loss = 0.38441673\n",
      "Iteration 148, loss = 0.38289210\n",
      "Iteration 149, loss = 0.38604177\n",
      "Iteration 150, loss = 0.38399701\n",
      "Iteration 151, loss = 0.38421091\n",
      "Iteration 152, loss = 0.38277766\n",
      "Iteration 153, loss = 0.38327044\n",
      "Iteration 154, loss = 0.38222274\n",
      "Iteration 155, loss = 0.38069165\n",
      "Iteration 156, loss = 0.38083925\n",
      "Iteration 157, loss = 0.38015984\n",
      "Iteration 158, loss = 0.38083378\n",
      "Iteration 159, loss = 0.37991997\n",
      "Iteration 160, loss = 0.38276927\n",
      "Iteration 161, loss = 0.38073477\n",
      "Iteration 162, loss = 0.37990674\n",
      "Iteration 163, loss = 0.38103543\n",
      "Iteration 164, loss = 0.37886635\n",
      "Iteration 165, loss = 0.37834689\n",
      "Iteration 166, loss = 0.37807156\n",
      "Iteration 167, loss = 0.37913441\n",
      "Iteration 168, loss = 0.37781293\n",
      "Iteration 169, loss = 0.37801934\n",
      "Iteration 170, loss = 0.37646548\n",
      "Iteration 171, loss = 0.37783724\n",
      "Iteration 172, loss = 0.37609157\n",
      "Iteration 173, loss = 0.37575624\n",
      "Iteration 174, loss = 0.37621792\n",
      "Iteration 175, loss = 0.37583310\n",
      "Iteration 176, loss = 0.37488856\n",
      "Iteration 177, loss = 0.37455685\n",
      "Iteration 178, loss = 0.37467531\n",
      "Iteration 179, loss = 0.37567862\n",
      "Iteration 180, loss = 0.37313641\n",
      "Iteration 181, loss = 0.37320993\n",
      "Iteration 182, loss = 0.37459024\n",
      "Iteration 183, loss = 0.37289811\n",
      "Iteration 184, loss = 0.37289206\n",
      "Iteration 185, loss = 0.37148260\n",
      "Iteration 186, loss = 0.37105476\n",
      "Iteration 187, loss = 0.37067835\n",
      "Iteration 188, loss = 0.37106411\n",
      "Iteration 189, loss = 0.36969202\n",
      "Iteration 190, loss = 0.37026984\n",
      "Iteration 191, loss = 0.36907781\n",
      "Iteration 192, loss = 0.36856196\n",
      "Iteration 193, loss = 0.36935572\n",
      "Iteration 194, loss = 0.37007741\n",
      "Iteration 195, loss = 0.37048380\n",
      "Iteration 196, loss = 0.36867469\n",
      "Iteration 197, loss = 0.36688276\n",
      "Iteration 198, loss = 0.36823128\n",
      "Iteration 199, loss = 0.36787733\n",
      "Iteration 200, loss = 0.36643279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.73997609\n",
      "Iteration 2, loss = 0.79184482\n",
      "Iteration 3, loss = 0.73416731\n",
      "Iteration 4, loss = 0.68881877\n",
      "Iteration 5, loss = 0.69138583\n",
      "Iteration 6, loss = 0.61791298\n",
      "Iteration 7, loss = 0.64437626\n",
      "Iteration 8, loss = 0.66831869\n",
      "Iteration 9, loss = 0.64454372\n",
      "Iteration 10, loss = 0.63027661\n",
      "Iteration 11, loss = 0.61785074\n",
      "Iteration 12, loss = 0.60240517\n",
      "Iteration 13, loss = 0.60881640\n",
      "Iteration 14, loss = 0.64849220\n",
      "Iteration 15, loss = 0.61825004\n",
      "Iteration 16, loss = 0.60945157\n",
      "Iteration 17, loss = 0.54788618\n",
      "Iteration 18, loss = 0.60772501\n",
      "Iteration 19, loss = 0.61393380\n",
      "Iteration 20, loss = 0.55349584\n",
      "Iteration 21, loss = 0.59064627\n",
      "Iteration 22, loss = 0.57384355\n",
      "Iteration 23, loss = 0.56151065\n",
      "Iteration 24, loss = 0.56336212\n",
      "Iteration 25, loss = 0.61269882\n",
      "Iteration 26, loss = 0.53578881\n",
      "Iteration 27, loss = 0.49856692\n",
      "Iteration 28, loss = 0.54506613\n",
      "Iteration 29, loss = 0.55255622\n",
      "Iteration 30, loss = 0.53936322\n",
      "Iteration 31, loss = 0.50531860\n",
      "Iteration 32, loss = 0.56013217\n",
      "Iteration 33, loss = 0.52793688\n",
      "Iteration 34, loss = 0.51137526\n",
      "Iteration 35, loss = 0.52683326\n",
      "Iteration 36, loss = 0.50975834\n",
      "Iteration 37, loss = 0.53488025\n",
      "Iteration 38, loss = 0.51706193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "   Number of Hidden Neurons  Accuracy\n",
      "0                         5  0.791375\n",
      "1                        10  0.803030\n",
      "2                        18  0.805361\n",
      "3                        20  0.816434\n",
      "4                        50  0.806527\n",
      "5                       100  0.809441\n",
      "6                       500  0.761072\n",
      "Classification Report for 5 neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       856\n",
      "           1       0.77      0.83      0.80       860\n",
      "\n",
      "    accuracy                           0.79      1716\n",
      "   macro avg       0.79      0.79      0.79      1716\n",
      "weighted avg       0.79      0.79      0.79      1716\n",
      "\n",
      "Classification Report for 10 neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80       856\n",
      "           1       0.78      0.84      0.81       860\n",
      "\n",
      "    accuracy                           0.80      1716\n",
      "   macro avg       0.80      0.80      0.80      1716\n",
      "weighted avg       0.80      0.80      0.80      1716\n",
      "\n",
      "Classification Report for 18 neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.75      0.79       856\n",
      "           1       0.77      0.86      0.82       860\n",
      "\n",
      "    accuracy                           0.81      1716\n",
      "   macro avg       0.81      0.81      0.80      1716\n",
      "weighted avg       0.81      0.81      0.80      1716\n",
      "\n",
      "Classification Report for 20 neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.77      0.81       856\n",
      "           1       0.79      0.87      0.83       860\n",
      "\n",
      "    accuracy                           0.82      1716\n",
      "   macro avg       0.82      0.82      0.82      1716\n",
      "weighted avg       0.82      0.82      0.82      1716\n",
      "\n",
      "Classification Report for 50 neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80       856\n",
      "           1       0.80      0.83      0.81       860\n",
      "\n",
      "    accuracy                           0.81      1716\n",
      "   macro avg       0.81      0.81      0.81      1716\n",
      "weighted avg       0.81      0.81      0.81      1716\n",
      "\n",
      "Classification Report for 100 neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81       856\n",
      "           1       0.81      0.81      0.81       860\n",
      "\n",
      "    accuracy                           0.81      1716\n",
      "   macro avg       0.81      0.81      0.81      1716\n",
      "weighted avg       0.81      0.81      0.81      1716\n",
      "\n",
      "Classification Report for 500 neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       856\n",
      "           1       0.73      0.84      0.78       860\n",
      "\n",
      "    accuracy                           0.76      1716\n",
      "   macro avg       0.77      0.76      0.76      1716\n",
      "weighted avg       0.77      0.76      0.76      1716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_1=[5,10,18,20,50,100,500]\n",
    "accuracy_score_test_1=[]\n",
    "#conf_matrices_1 = []\n",
    "class_reports_1 = []\n",
    "for i in test_1:\n",
    "    mlp_test_1 = MLPClassifier(hidden_layer_sizes=(i,), activation='logistic', alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1, verbose=True)\n",
    "    mlp_test_1.fit(X_train,y_train)\n",
    "    predictions = mlp_test_1.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_score_test_1.append((i,accuracy))\n",
    "   # conf_matrices_1.append((i, confusion_matrix(y_test, predictions)))\n",
    "    class_reports_1.append((i, classification_report(y_test, predictions)))\n",
    "ac_1 = pd.DataFrame(accuracy_score_test_1, columns=['Number of Hidden Neurons', 'Accuracy'])\n",
    "print(ac_1)\n",
    "#for neurons, matrix in conf_matrices_1:\n",
    " #   print(f\"Confusion Matrix for {neurons} neurons:\")\n",
    "  #  print(matrix)\n",
    "for neurons, report in class_reports_1:\n",
    "    print(f\"Classification Report for {neurons} neurons:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbd6c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69689726\n",
      "Iteration 2, loss = 0.64363298\n",
      "Iteration 3, loss = 0.50798616\n",
      "Iteration 4, loss = 0.48221241\n",
      "Iteration 5, loss = 0.47990962\n",
      "Iteration 6, loss = 0.47687864\n",
      "Iteration 7, loss = 0.47507266\n",
      "Iteration 8, loss = 0.47279275\n",
      "Iteration 9, loss = 0.47176134\n",
      "Iteration 10, loss = 0.46902170\n",
      "Iteration 11, loss = 0.46575271\n",
      "Iteration 12, loss = 0.46341054\n",
      "Iteration 13, loss = 0.46165456\n",
      "Iteration 14, loss = 0.45997748\n",
      "Iteration 15, loss = 0.45794860\n",
      "Iteration 16, loss = 0.45788350\n",
      "Iteration 17, loss = 0.45480071\n",
      "Iteration 18, loss = 0.45225570\n",
      "Iteration 19, loss = 0.45091381\n",
      "Iteration 20, loss = 0.44840108\n",
      "Iteration 21, loss = 0.44659194\n",
      "Iteration 22, loss = 0.44235553\n",
      "Iteration 23, loss = 0.44021686\n",
      "Iteration 24, loss = 0.43836934\n",
      "Iteration 25, loss = 0.43499839\n",
      "Iteration 26, loss = 0.43323929\n",
      "Iteration 27, loss = 0.43071069\n",
      "Iteration 28, loss = 0.42960747\n",
      "Iteration 29, loss = 0.42756395\n",
      "Iteration 30, loss = 0.42547785\n",
      "Iteration 31, loss = 0.42489483\n",
      "Iteration 32, loss = 0.42364962\n",
      "Iteration 33, loss = 0.42151864\n",
      "Iteration 34, loss = 0.42217360\n",
      "Iteration 35, loss = 0.42143500\n",
      "Iteration 36, loss = 0.41990428\n",
      "Iteration 37, loss = 0.41848891\n",
      "Iteration 38, loss = 0.41878954\n",
      "Iteration 39, loss = 0.41834394\n",
      "Iteration 40, loss = 0.41842468\n",
      "Iteration 41, loss = 0.41607412\n",
      "Iteration 42, loss = 0.41568987\n",
      "Iteration 43, loss = 0.41427177\n",
      "Iteration 44, loss = 0.41457467\n",
      "Iteration 45, loss = 0.41450846\n",
      "Iteration 46, loss = 0.41235403\n",
      "Iteration 47, loss = 0.41172431\n",
      "Iteration 48, loss = 0.41082309\n",
      "Iteration 49, loss = 0.41034564\n",
      "Iteration 50, loss = 0.40956687\n",
      "Iteration 51, loss = 0.40923647\n",
      "Iteration 52, loss = 0.40712538\n",
      "Iteration 53, loss = 0.40760073\n",
      "Iteration 54, loss = 0.40586596\n",
      "Iteration 55, loss = 0.40791011\n",
      "Iteration 56, loss = 0.40640710\n",
      "Iteration 57, loss = 0.40612384\n",
      "Iteration 58, loss = 0.40444630\n",
      "Iteration 59, loss = 0.40543675\n",
      "Iteration 60, loss = 0.40245703\n",
      "Iteration 61, loss = 0.40305364\n",
      "Iteration 62, loss = 0.40266429\n",
      "Iteration 63, loss = 0.40213050\n",
      "Iteration 64, loss = 0.40137896\n",
      "Iteration 65, loss = 0.40098902\n",
      "Iteration 66, loss = 0.40102099\n",
      "Iteration 67, loss = 0.39944301\n",
      "Iteration 68, loss = 0.39805255\n",
      "Iteration 69, loss = 0.39755593\n",
      "Iteration 70, loss = 0.39703370\n",
      "Iteration 71, loss = 0.39860019\n",
      "Iteration 72, loss = 0.39578381\n",
      "Iteration 73, loss = 0.39642524\n",
      "Iteration 74, loss = 0.39416717\n",
      "Iteration 75, loss = 0.39465821\n",
      "Iteration 76, loss = 0.39419817\n",
      "Iteration 77, loss = 0.39300165\n",
      "Iteration 78, loss = 0.39349345\n",
      "Iteration 79, loss = 0.39316485\n",
      "Iteration 80, loss = 0.39194548\n",
      "Iteration 81, loss = 0.39013672\n",
      "Iteration 82, loss = 0.39095098\n",
      "Iteration 83, loss = 0.38950630\n",
      "Iteration 84, loss = 0.38949592\n",
      "Iteration 85, loss = 0.38911127\n",
      "Iteration 86, loss = 0.38818100\n",
      "Iteration 87, loss = 0.38699312\n",
      "Iteration 88, loss = 0.38833290\n",
      "Iteration 89, loss = 0.38736626\n",
      "Iteration 90, loss = 0.38766883\n",
      "Iteration 91, loss = 0.38586597\n",
      "Iteration 92, loss = 0.38568019\n",
      "Iteration 93, loss = 0.38565707\n",
      "Iteration 94, loss = 0.38368511\n",
      "Iteration 95, loss = 0.38466190\n",
      "Iteration 96, loss = 0.38436937\n",
      "Iteration 97, loss = 0.38371885\n",
      "Iteration 98, loss = 0.38454747\n",
      "Iteration 99, loss = 0.38191949\n",
      "Iteration 100, loss = 0.38370781\n",
      "Iteration 101, loss = 0.38208898\n",
      "Iteration 102, loss = 0.38293557\n",
      "Iteration 103, loss = 0.38253726\n",
      "Iteration 104, loss = 0.38149521\n",
      "Iteration 105, loss = 0.38016859\n",
      "Iteration 106, loss = 0.37969137\n",
      "Iteration 107, loss = 0.37938123\n",
      "Iteration 108, loss = 0.37971659\n",
      "Iteration 109, loss = 0.37916746\n",
      "Iteration 110, loss = 0.37884469\n",
      "Iteration 111, loss = 0.37863599\n",
      "Iteration 112, loss = 0.37767353\n",
      "Iteration 113, loss = 0.37877153\n",
      "Iteration 114, loss = 0.37677306\n",
      "Iteration 115, loss = 0.37735222\n",
      "Iteration 116, loss = 0.37790013\n",
      "Iteration 117, loss = 0.37630595\n",
      "Iteration 118, loss = 0.37541729\n",
      "Iteration 119, loss = 0.37511603\n",
      "Iteration 120, loss = 0.37407862\n",
      "Iteration 121, loss = 0.37420431\n",
      "Iteration 122, loss = 0.37374907\n",
      "Iteration 123, loss = 0.37212734\n",
      "Iteration 124, loss = 0.37226983\n",
      "Iteration 125, loss = 0.37222474\n",
      "Iteration 126, loss = 0.37267396\n",
      "Iteration 127, loss = 0.37322991\n",
      "Iteration 128, loss = 0.37178460\n",
      "Iteration 129, loss = 0.37261458\n",
      "Iteration 130, loss = 0.37266246\n",
      "Iteration 131, loss = 0.37008370\n",
      "Iteration 132, loss = 0.36998418\n",
      "Iteration 133, loss = 0.37043145\n",
      "Iteration 134, loss = 0.36998235\n",
      "Iteration 135, loss = 0.37019719\n",
      "Iteration 136, loss = 0.37037356\n",
      "Iteration 137, loss = 0.36985678\n",
      "Iteration 138, loss = 0.36887971\n",
      "Iteration 139, loss = 0.37226082\n",
      "Iteration 140, loss = 0.36854281\n",
      "Iteration 141, loss = 0.36917172\n",
      "Iteration 142, loss = 0.36864642\n",
      "Iteration 143, loss = 0.36764466\n",
      "Iteration 144, loss = 0.36716679\n",
      "Iteration 145, loss = 0.36638380\n",
      "Iteration 146, loss = 0.36625051\n",
      "Iteration 147, loss = 0.36695522\n",
      "Iteration 148, loss = 0.36473257\n",
      "Iteration 149, loss = 0.36580279\n",
      "Iteration 150, loss = 0.36456901\n",
      "Iteration 151, loss = 0.36449605\n",
      "Iteration 152, loss = 0.36438643\n",
      "Iteration 153, loss = 0.36380316\n",
      "Iteration 154, loss = 0.36358157\n",
      "Iteration 155, loss = 0.36417540\n",
      "Iteration 156, loss = 0.36426865\n",
      "Iteration 157, loss = 0.36419114\n",
      "Iteration 158, loss = 0.36350847\n",
      "Iteration 159, loss = 0.36231532\n",
      "Iteration 160, loss = 0.36373001\n",
      "Iteration 161, loss = 0.36540026\n",
      "Iteration 162, loss = 0.36254835\n",
      "Iteration 163, loss = 0.36289496\n",
      "Iteration 164, loss = 0.36163014\n",
      "Iteration 165, loss = 0.36081250\n",
      "Iteration 166, loss = 0.36237529\n",
      "Iteration 167, loss = 0.36054011\n",
      "Iteration 168, loss = 0.36221476\n",
      "Iteration 169, loss = 0.36136562\n",
      "Iteration 170, loss = 0.36338369\n",
      "Iteration 171, loss = 0.36074383\n",
      "Iteration 172, loss = 0.36000685\n",
      "Iteration 173, loss = 0.36132729\n",
      "Iteration 174, loss = 0.35947138\n",
      "Iteration 175, loss = 0.35969695\n",
      "Iteration 176, loss = 0.35890839\n",
      "Iteration 177, loss = 0.35935014\n",
      "Iteration 178, loss = 0.35995954\n",
      "Iteration 179, loss = 0.35870821\n",
      "Iteration 180, loss = 0.35793776\n",
      "Iteration 181, loss = 0.35882345\n",
      "Iteration 182, loss = 0.35886434\n",
      "Iteration 183, loss = 0.35800376\n",
      "Iteration 184, loss = 0.35878749\n",
      "Iteration 185, loss = 0.35706457\n",
      "Iteration 186, loss = 0.35675338\n",
      "Iteration 187, loss = 0.35614919\n",
      "Iteration 188, loss = 0.35618102\n",
      "Iteration 189, loss = 0.35686997\n",
      "Iteration 190, loss = 0.35609034\n",
      "Iteration 191, loss = 0.35535388\n",
      "Iteration 192, loss = 0.35528588\n",
      "Iteration 193, loss = 0.35637475\n",
      "Iteration 194, loss = 0.35524739\n",
      "Iteration 195, loss = 0.35536833\n",
      "Iteration 196, loss = 0.35533589\n",
      "Iteration 197, loss = 0.35381606\n",
      "Iteration 198, loss = 0.35350601\n",
      "Iteration 199, loss = 0.35390584\n",
      "Iteration 200, loss = 0.35307547\n",
      "Iteration 1, loss = 0.68623730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.57300243\n",
      "Iteration 3, loss = 0.48759661\n",
      "Iteration 4, loss = 0.48192451\n",
      "Iteration 5, loss = 0.47699441\n",
      "Iteration 6, loss = 0.47640011\n",
      "Iteration 7, loss = 0.47337306\n",
      "Iteration 8, loss = 0.47121585\n",
      "Iteration 9, loss = 0.46945677\n",
      "Iteration 10, loss = 0.46824155\n",
      "Iteration 11, loss = 0.46684351\n",
      "Iteration 12, loss = 0.46497218\n",
      "Iteration 13, loss = 0.46221919\n",
      "Iteration 14, loss = 0.46129623\n",
      "Iteration 15, loss = 0.45743890\n",
      "Iteration 16, loss = 0.45458559\n",
      "Iteration 17, loss = 0.45384599\n",
      "Iteration 18, loss = 0.45056161\n",
      "Iteration 19, loss = 0.44796305\n",
      "Iteration 20, loss = 0.44535622\n",
      "Iteration 21, loss = 0.44361629\n",
      "Iteration 22, loss = 0.44241152\n",
      "Iteration 23, loss = 0.43970282\n",
      "Iteration 24, loss = 0.43771869\n",
      "Iteration 25, loss = 0.43513711\n",
      "Iteration 26, loss = 0.43351694\n",
      "Iteration 27, loss = 0.43186501\n",
      "Iteration 28, loss = 0.42804415\n",
      "Iteration 29, loss = 0.42512638\n",
      "Iteration 30, loss = 0.42343595\n",
      "Iteration 31, loss = 0.42011609\n",
      "Iteration 32, loss = 0.41825712\n",
      "Iteration 33, loss = 0.41610789\n",
      "Iteration 34, loss = 0.41445255\n",
      "Iteration 35, loss = 0.41323592\n",
      "Iteration 36, loss = 0.41318726\n",
      "Iteration 37, loss = 0.41111769\n",
      "Iteration 38, loss = 0.40910295\n",
      "Iteration 39, loss = 0.40906828\n",
      "Iteration 40, loss = 0.40676353\n",
      "Iteration 41, loss = 0.40757964\n",
      "Iteration 42, loss = 0.40433245\n",
      "Iteration 43, loss = 0.40466354\n",
      "Iteration 44, loss = 0.40362892\n",
      "Iteration 45, loss = 0.40259627\n",
      "Iteration 46, loss = 0.40179569\n",
      "Iteration 47, loss = 0.40256143\n",
      "Iteration 48, loss = 0.40095191\n",
      "Iteration 49, loss = 0.39844945\n",
      "Iteration 50, loss = 0.39952537\n",
      "Iteration 51, loss = 0.39883516\n",
      "Iteration 52, loss = 0.39739119\n",
      "Iteration 53, loss = 0.39593674\n",
      "Iteration 54, loss = 0.39515315\n",
      "Iteration 55, loss = 0.39654193\n",
      "Iteration 56, loss = 0.39471899\n",
      "Iteration 57, loss = 0.39339290\n",
      "Iteration 58, loss = 0.39347151\n",
      "Iteration 59, loss = 0.39087310\n",
      "Iteration 60, loss = 0.39413854\n",
      "Iteration 61, loss = 0.39114968\n",
      "Iteration 62, loss = 0.38867106\n",
      "Iteration 63, loss = 0.38769778\n",
      "Iteration 64, loss = 0.38668117\n",
      "Iteration 65, loss = 0.38774398\n",
      "Iteration 66, loss = 0.38781588\n",
      "Iteration 67, loss = 0.38563276\n",
      "Iteration 68, loss = 0.38554021\n",
      "Iteration 69, loss = 0.38446668\n",
      "Iteration 70, loss = 0.38250504\n",
      "Iteration 71, loss = 0.38174523\n",
      "Iteration 72, loss = 0.38254055\n",
      "Iteration 73, loss = 0.38018762\n",
      "Iteration 74, loss = 0.37990025\n",
      "Iteration 75, loss = 0.37820580\n",
      "Iteration 76, loss = 0.37855671\n",
      "Iteration 77, loss = 0.37733896\n",
      "Iteration 78, loss = 0.37609320\n",
      "Iteration 79, loss = 0.37628778\n",
      "Iteration 80, loss = 0.37474844\n",
      "Iteration 81, loss = 0.37255764\n",
      "Iteration 82, loss = 0.37385340\n",
      "Iteration 83, loss = 0.37160246\n",
      "Iteration 84, loss = 0.37284668\n",
      "Iteration 85, loss = 0.37303339\n",
      "Iteration 86, loss = 0.37253470\n",
      "Iteration 87, loss = 0.36965629\n",
      "Iteration 88, loss = 0.36862587\n",
      "Iteration 89, loss = 0.37040034\n",
      "Iteration 90, loss = 0.36835792\n",
      "Iteration 91, loss = 0.36942731\n",
      "Iteration 92, loss = 0.36669973\n",
      "Iteration 93, loss = 0.36775294\n",
      "Iteration 94, loss = 0.36563572\n",
      "Iteration 95, loss = 0.36498400\n",
      "Iteration 96, loss = 0.36569390\n",
      "Iteration 97, loss = 0.36405786\n",
      "Iteration 98, loss = 0.36353035\n",
      "Iteration 99, loss = 0.36525321\n",
      "Iteration 100, loss = 0.36635324\n",
      "Iteration 101, loss = 0.36516201\n",
      "Iteration 102, loss = 0.36298341\n",
      "Iteration 103, loss = 0.36074879\n",
      "Iteration 104, loss = 0.36231934\n",
      "Iteration 105, loss = 0.36050248\n",
      "Iteration 106, loss = 0.35999421\n",
      "Iteration 107, loss = 0.36009930\n",
      "Iteration 108, loss = 0.36006397\n",
      "Iteration 109, loss = 0.35837868\n",
      "Iteration 110, loss = 0.35795769\n",
      "Iteration 111, loss = 0.35790419\n",
      "Iteration 112, loss = 0.35853533\n",
      "Iteration 113, loss = 0.35549730\n",
      "Iteration 114, loss = 0.35784744\n",
      "Iteration 115, loss = 0.35713830\n",
      "Iteration 116, loss = 0.35360756\n",
      "Iteration 117, loss = 0.35779902\n",
      "Iteration 118, loss = 0.35538499\n",
      "Iteration 119, loss = 0.35389163\n",
      "Iteration 120, loss = 0.35325027\n",
      "Iteration 121, loss = 0.35190795\n",
      "Iteration 122, loss = 0.35027673\n",
      "Iteration 123, loss = 0.35004945\n",
      "Iteration 124, loss = 0.35082101\n",
      "Iteration 125, loss = 0.35086582\n",
      "Iteration 126, loss = 0.34943924\n",
      "Iteration 127, loss = 0.35101046\n",
      "Iteration 128, loss = 0.34780193\n",
      "Iteration 129, loss = 0.34682653\n",
      "Iteration 130, loss = 0.34881610\n",
      "Iteration 131, loss = 0.34705387\n",
      "Iteration 132, loss = 0.34535507\n",
      "Iteration 133, loss = 0.34616038\n",
      "Iteration 134, loss = 0.34652212\n",
      "Iteration 135, loss = 0.34614406\n",
      "Iteration 136, loss = 0.34464121\n",
      "Iteration 137, loss = 0.34514262\n",
      "Iteration 138, loss = 0.34133964\n",
      "Iteration 139, loss = 0.34318546\n",
      "Iteration 140, loss = 0.34239349\n",
      "Iteration 141, loss = 0.34208701\n",
      "Iteration 142, loss = 0.34137083\n",
      "Iteration 143, loss = 0.34023165\n",
      "Iteration 144, loss = 0.34131911\n",
      "Iteration 145, loss = 0.34039891\n",
      "Iteration 146, loss = 0.33955644\n",
      "Iteration 147, loss = 0.33905032\n",
      "Iteration 148, loss = 0.34167943\n",
      "Iteration 149, loss = 0.33817265\n",
      "Iteration 150, loss = 0.33780982\n",
      "Iteration 151, loss = 0.33864819\n",
      "Iteration 152, loss = 0.33730786\n",
      "Iteration 153, loss = 0.33868045\n",
      "Iteration 154, loss = 0.33695347\n",
      "Iteration 155, loss = 0.33575590\n",
      "Iteration 156, loss = 0.33493400\n",
      "Iteration 157, loss = 0.33546687\n",
      "Iteration 158, loss = 0.33518898\n",
      "Iteration 159, loss = 0.33435563\n",
      "Iteration 160, loss = 0.33389288\n",
      "Iteration 161, loss = 0.33555404\n",
      "Iteration 162, loss = 0.33318863\n",
      "Iteration 163, loss = 0.33510861\n",
      "Iteration 164, loss = 0.33415496\n",
      "Iteration 165, loss = 0.33189385\n",
      "Iteration 166, loss = 0.33181320\n",
      "Iteration 167, loss = 0.33057216\n",
      "Iteration 168, loss = 0.33190736\n",
      "Iteration 169, loss = 0.33227295\n",
      "Iteration 170, loss = 0.33144384\n",
      "Iteration 171, loss = 0.33044724\n",
      "Iteration 172, loss = 0.33122815\n",
      "Iteration 173, loss = 0.33090225\n",
      "Iteration 174, loss = 0.33140723\n",
      "Iteration 175, loss = 0.33052585\n",
      "Iteration 176, loss = 0.32841767\n",
      "Iteration 177, loss = 0.33026743\n",
      "Iteration 178, loss = 0.32947953\n",
      "Iteration 179, loss = 0.32859680\n",
      "Iteration 180, loss = 0.32796606\n",
      "Iteration 181, loss = 0.32818649\n",
      "Iteration 182, loss = 0.33014947\n",
      "Iteration 183, loss = 0.32738025\n",
      "Iteration 184, loss = 0.32756714\n",
      "Iteration 185, loss = 0.32724685\n",
      "Iteration 186, loss = 0.32768578\n",
      "Iteration 187, loss = 0.32633493\n",
      "Iteration 188, loss = 0.32585208\n",
      "Iteration 189, loss = 0.32369224\n",
      "Iteration 190, loss = 0.32592226\n",
      "Iteration 191, loss = 0.32342359\n",
      "Iteration 192, loss = 0.32564778\n",
      "Iteration 193, loss = 0.32442628\n",
      "Iteration 194, loss = 0.32408513\n",
      "Iteration 195, loss = 0.32536655\n",
      "Iteration 196, loss = 0.32456662\n",
      "Iteration 197, loss = 0.32379543\n",
      "Iteration 198, loss = 0.32426656\n",
      "Iteration 199, loss = 0.32349182\n",
      "Iteration 200, loss = 0.32329769\n",
      "Iteration 1, loss = 0.69324459\n",
      "Iteration 2, loss = 0.62623939\n",
      "Iteration 3, loss = 0.49670314\n",
      "Iteration 4, loss = 0.48083371\n",
      "Iteration 5, loss = 0.47932495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.47657977\n",
      "Iteration 7, loss = 0.47484759\n",
      "Iteration 8, loss = 0.47233827\n",
      "Iteration 9, loss = 0.47302351\n",
      "Iteration 10, loss = 0.46972259\n",
      "Iteration 11, loss = 0.46771771\n",
      "Iteration 12, loss = 0.46598731\n",
      "Iteration 13, loss = 0.46527711\n",
      "Iteration 14, loss = 0.46259353\n",
      "Iteration 15, loss = 0.46255401\n",
      "Iteration 16, loss = 0.46033186\n",
      "Iteration 17, loss = 0.45859892\n",
      "Iteration 18, loss = 0.45714770\n",
      "Iteration 19, loss = 0.45515925\n",
      "Iteration 20, loss = 0.45458172\n",
      "Iteration 21, loss = 0.45178040\n",
      "Iteration 22, loss = 0.45066809\n",
      "Iteration 23, loss = 0.44932856\n",
      "Iteration 24, loss = 0.44643056\n",
      "Iteration 25, loss = 0.44530028\n",
      "Iteration 26, loss = 0.44290683\n",
      "Iteration 27, loss = 0.43969874\n",
      "Iteration 28, loss = 0.43776549\n",
      "Iteration 29, loss = 0.43528686\n",
      "Iteration 30, loss = 0.43219677\n",
      "Iteration 31, loss = 0.42879553\n",
      "Iteration 32, loss = 0.42814313\n",
      "Iteration 33, loss = 0.42306449\n",
      "Iteration 34, loss = 0.42143226\n",
      "Iteration 35, loss = 0.41884066\n",
      "Iteration 36, loss = 0.41670293\n",
      "Iteration 37, loss = 0.41535245\n",
      "Iteration 38, loss = 0.41508397\n",
      "Iteration 39, loss = 0.41063626\n",
      "Iteration 40, loss = 0.40947490\n",
      "Iteration 41, loss = 0.40813428\n",
      "Iteration 42, loss = 0.40716266\n",
      "Iteration 43, loss = 0.40516467\n",
      "Iteration 44, loss = 0.40539675\n",
      "Iteration 45, loss = 0.40257177\n",
      "Iteration 46, loss = 0.40432869\n",
      "Iteration 47, loss = 0.40048147\n",
      "Iteration 48, loss = 0.39940049\n",
      "Iteration 49, loss = 0.40100130\n",
      "Iteration 50, loss = 0.39848202\n",
      "Iteration 51, loss = 0.39891374\n",
      "Iteration 52, loss = 0.40024091\n",
      "Iteration 53, loss = 0.39668703\n",
      "Iteration 54, loss = 0.39561792\n",
      "Iteration 55, loss = 0.39374653\n",
      "Iteration 56, loss = 0.39484267\n",
      "Iteration 57, loss = 0.39273712\n",
      "Iteration 58, loss = 0.39302713\n",
      "Iteration 59, loss = 0.39219961\n",
      "Iteration 60, loss = 0.39267239\n",
      "Iteration 61, loss = 0.39087092\n",
      "Iteration 62, loss = 0.39251975\n",
      "Iteration 63, loss = 0.38990801\n",
      "Iteration 64, loss = 0.38776021\n",
      "Iteration 65, loss = 0.38894883\n",
      "Iteration 66, loss = 0.38707131\n",
      "Iteration 67, loss = 0.38524509\n",
      "Iteration 68, loss = 0.38524233\n",
      "Iteration 69, loss = 0.38525317\n",
      "Iteration 70, loss = 0.38452591\n",
      "Iteration 71, loss = 0.38382949\n",
      "Iteration 72, loss = 0.38313635\n",
      "Iteration 73, loss = 0.38469150\n",
      "Iteration 74, loss = 0.38154141\n",
      "Iteration 75, loss = 0.38008370\n",
      "Iteration 76, loss = 0.38063362\n",
      "Iteration 77, loss = 0.37894533\n",
      "Iteration 78, loss = 0.37873139\n",
      "Iteration 79, loss = 0.38031494\n",
      "Iteration 80, loss = 0.37800998\n",
      "Iteration 81, loss = 0.37746499\n",
      "Iteration 82, loss = 0.37598234\n",
      "Iteration 83, loss = 0.37527724\n",
      "Iteration 84, loss = 0.37584661\n",
      "Iteration 85, loss = 0.37333355\n",
      "Iteration 86, loss = 0.37466790\n",
      "Iteration 87, loss = 0.37359969\n",
      "Iteration 88, loss = 0.37022694\n",
      "Iteration 89, loss = 0.37134395\n",
      "Iteration 90, loss = 0.36863306\n",
      "Iteration 91, loss = 0.36866175\n",
      "Iteration 92, loss = 0.36945283\n",
      "Iteration 93, loss = 0.36823743\n",
      "Iteration 94, loss = 0.36599669\n",
      "Iteration 95, loss = 0.36623646\n",
      "Iteration 96, loss = 0.36362389\n",
      "Iteration 97, loss = 0.36596638\n",
      "Iteration 98, loss = 0.36327130\n",
      "Iteration 99, loss = 0.36300211\n",
      "Iteration 100, loss = 0.36487760\n",
      "Iteration 101, loss = 0.36377484\n",
      "Iteration 102, loss = 0.36032627\n",
      "Iteration 103, loss = 0.36068398\n",
      "Iteration 104, loss = 0.35783906\n",
      "Iteration 105, loss = 0.35965827\n",
      "Iteration 106, loss = 0.35925869\n",
      "Iteration 107, loss = 0.35845275\n",
      "Iteration 108, loss = 0.35797650\n",
      "Iteration 109, loss = 0.35688622\n",
      "Iteration 110, loss = 0.35693711\n",
      "Iteration 111, loss = 0.35457047\n",
      "Iteration 112, loss = 0.35539130\n",
      "Iteration 113, loss = 0.35568610\n",
      "Iteration 114, loss = 0.35478463\n",
      "Iteration 115, loss = 0.35254298\n",
      "Iteration 116, loss = 0.35616736\n",
      "Iteration 117, loss = 0.35253342\n",
      "Iteration 118, loss = 0.35468471\n",
      "Iteration 119, loss = 0.35135648\n",
      "Iteration 120, loss = 0.35156081\n",
      "Iteration 121, loss = 0.35218935\n",
      "Iteration 122, loss = 0.35065770\n",
      "Iteration 123, loss = 0.34958401\n",
      "Iteration 124, loss = 0.35050688\n",
      "Iteration 125, loss = 0.34874293\n",
      "Iteration 126, loss = 0.34615376\n",
      "Iteration 127, loss = 0.34862706\n",
      "Iteration 128, loss = 0.34644536\n",
      "Iteration 129, loss = 0.34772401\n",
      "Iteration 130, loss = 0.34640453\n",
      "Iteration 131, loss = 0.34772176\n",
      "Iteration 132, loss = 0.34492413\n",
      "Iteration 133, loss = 0.34415707\n",
      "Iteration 134, loss = 0.34260561\n",
      "Iteration 135, loss = 0.34377309\n",
      "Iteration 136, loss = 0.34260100\n",
      "Iteration 137, loss = 0.34162334\n",
      "Iteration 138, loss = 0.34062477\n",
      "Iteration 139, loss = 0.34156437\n",
      "Iteration 140, loss = 0.34033364\n",
      "Iteration 141, loss = 0.34129574\n",
      "Iteration 142, loss = 0.34081893\n",
      "Iteration 143, loss = 0.34016952\n",
      "Iteration 144, loss = 0.33751762\n",
      "Iteration 145, loss = 0.33734989\n",
      "Iteration 146, loss = 0.33726202\n",
      "Iteration 147, loss = 0.33737304\n",
      "Iteration 148, loss = 0.33684168\n",
      "Iteration 149, loss = 0.33693757\n",
      "Iteration 150, loss = 0.33577758\n",
      "Iteration 151, loss = 0.33479078\n",
      "Iteration 152, loss = 0.33366474\n",
      "Iteration 153, loss = 0.33329401\n",
      "Iteration 154, loss = 0.33232251\n",
      "Iteration 155, loss = 0.33217400\n",
      "Iteration 156, loss = 0.33170983\n",
      "Iteration 157, loss = 0.33130173\n",
      "Iteration 158, loss = 0.33163682\n",
      "Iteration 159, loss = 0.32960233\n",
      "Iteration 160, loss = 0.32947199\n",
      "Iteration 161, loss = 0.32919039\n",
      "Iteration 162, loss = 0.32795936\n",
      "Iteration 163, loss = 0.32803578\n",
      "Iteration 164, loss = 0.32804052\n",
      "Iteration 165, loss = 0.32785363\n",
      "Iteration 166, loss = 0.32655698\n",
      "Iteration 167, loss = 0.32567785\n",
      "Iteration 168, loss = 0.32440134\n",
      "Iteration 169, loss = 0.32356866\n",
      "Iteration 170, loss = 0.32513804\n",
      "Iteration 171, loss = 0.32517009\n",
      "Iteration 172, loss = 0.32193853\n",
      "Iteration 173, loss = 0.32324274\n",
      "Iteration 174, loss = 0.32237415\n",
      "Iteration 175, loss = 0.32102113\n",
      "Iteration 176, loss = 0.32022330\n",
      "Iteration 177, loss = 0.31993444\n",
      "Iteration 178, loss = 0.31953031\n",
      "Iteration 179, loss = 0.31909188\n",
      "Iteration 180, loss = 0.31893485\n",
      "Iteration 181, loss = 0.31798177\n",
      "Iteration 182, loss = 0.31637413\n",
      "Iteration 183, loss = 0.31534506\n",
      "Iteration 184, loss = 0.31653779\n",
      "Iteration 185, loss = 0.31624158\n",
      "Iteration 186, loss = 0.31426516\n",
      "Iteration 187, loss = 0.31427271\n",
      "Iteration 188, loss = 0.31236310\n",
      "Iteration 189, loss = 0.31267127\n",
      "Iteration 190, loss = 0.31254958\n",
      "Iteration 191, loss = 0.31181734\n",
      "Iteration 192, loss = 0.31191582\n",
      "Iteration 193, loss = 0.31097516\n",
      "Iteration 194, loss = 0.31198406\n",
      "Iteration 195, loss = 0.30951011\n",
      "Iteration 196, loss = 0.30808255\n",
      "Iteration 197, loss = 0.30950142\n",
      "Iteration 198, loss = 0.30858006\n",
      "Iteration 199, loss = 0.30875952\n",
      "Iteration 200, loss = 0.30782533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69182474\n",
      "Iteration 2, loss = 0.61338285\n",
      "Iteration 3, loss = 0.50191009\n",
      "Iteration 4, loss = 0.48193306\n",
      "Iteration 5, loss = 0.48004165\n",
      "Iteration 6, loss = 0.47932519\n",
      "Iteration 7, loss = 0.47588524\n",
      "Iteration 8, loss = 0.47555471\n",
      "Iteration 9, loss = 0.47406699\n",
      "Iteration 10, loss = 0.47211598\n",
      "Iteration 11, loss = 0.47255100\n",
      "Iteration 12, loss = 0.46871235\n",
      "Iteration 13, loss = 0.46808019\n",
      "Iteration 14, loss = 0.46633394\n",
      "Iteration 15, loss = 0.46234648\n",
      "Iteration 16, loss = 0.46257093\n",
      "Iteration 17, loss = 0.46013528\n",
      "Iteration 18, loss = 0.45883427\n",
      "Iteration 19, loss = 0.45625507\n",
      "Iteration 20, loss = 0.45405762\n",
      "Iteration 21, loss = 0.45086004\n",
      "Iteration 22, loss = 0.44818046\n",
      "Iteration 23, loss = 0.44648795\n",
      "Iteration 24, loss = 0.44450405\n",
      "Iteration 25, loss = 0.44197423\n",
      "Iteration 26, loss = 0.44079248\n",
      "Iteration 27, loss = 0.43796066\n",
      "Iteration 28, loss = 0.43466426\n",
      "Iteration 29, loss = 0.43358445\n",
      "Iteration 30, loss = 0.43209044\n",
      "Iteration 31, loss = 0.42974107\n",
      "Iteration 32, loss = 0.42803301\n",
      "Iteration 33, loss = 0.42685654\n",
      "Iteration 34, loss = 0.42429534\n",
      "Iteration 35, loss = 0.42263271\n",
      "Iteration 36, loss = 0.41959535\n",
      "Iteration 37, loss = 0.42118288\n",
      "Iteration 38, loss = 0.41555104\n",
      "Iteration 39, loss = 0.41660027\n",
      "Iteration 40, loss = 0.41447204\n",
      "Iteration 41, loss = 0.41443788\n",
      "Iteration 42, loss = 0.41128220\n",
      "Iteration 43, loss = 0.41106654\n",
      "Iteration 44, loss = 0.41127468\n",
      "Iteration 45, loss = 0.40658137\n",
      "Iteration 46, loss = 0.40565958\n",
      "Iteration 47, loss = 0.40270455\n",
      "Iteration 48, loss = 0.40291598\n",
      "Iteration 49, loss = 0.40320709\n",
      "Iteration 50, loss = 0.40063560\n",
      "Iteration 51, loss = 0.39922759\n",
      "Iteration 52, loss = 0.39892031\n",
      "Iteration 53, loss = 0.39869366\n",
      "Iteration 54, loss = 0.39694953\n",
      "Iteration 55, loss = 0.39584334\n",
      "Iteration 56, loss = 0.39325007\n",
      "Iteration 57, loss = 0.39257204\n",
      "Iteration 58, loss = 0.39299013\n",
      "Iteration 59, loss = 0.39245289\n",
      "Iteration 60, loss = 0.39149519\n",
      "Iteration 61, loss = 0.39060927\n",
      "Iteration 62, loss = 0.38893485\n",
      "Iteration 63, loss = 0.38686484\n",
      "Iteration 64, loss = 0.38662775\n",
      "Iteration 65, loss = 0.38559120\n",
      "Iteration 66, loss = 0.38532538\n",
      "Iteration 67, loss = 0.38264327\n",
      "Iteration 68, loss = 0.38460700\n",
      "Iteration 69, loss = 0.38190597\n",
      "Iteration 70, loss = 0.38378609\n",
      "Iteration 71, loss = 0.38087307\n",
      "Iteration 72, loss = 0.37942050\n",
      "Iteration 73, loss = 0.37903095\n",
      "Iteration 74, loss = 0.37701778\n",
      "Iteration 75, loss = 0.37610528\n",
      "Iteration 76, loss = 0.37431086\n",
      "Iteration 77, loss = 0.37476935\n",
      "Iteration 78, loss = 0.37488458\n",
      "Iteration 79, loss = 0.37348985\n",
      "Iteration 80, loss = 0.37492908\n",
      "Iteration 81, loss = 0.37195375\n",
      "Iteration 82, loss = 0.37069515\n",
      "Iteration 83, loss = 0.37070436\n",
      "Iteration 84, loss = 0.36889809\n",
      "Iteration 85, loss = 0.36992261\n",
      "Iteration 86, loss = 0.36879208\n",
      "Iteration 87, loss = 0.36742066\n",
      "Iteration 88, loss = 0.36618401\n",
      "Iteration 89, loss = 0.36537739\n",
      "Iteration 90, loss = 0.36606009\n",
      "Iteration 91, loss = 0.36475541\n",
      "Iteration 92, loss = 0.36327520\n",
      "Iteration 93, loss = 0.36212511\n",
      "Iteration 94, loss = 0.36113930\n",
      "Iteration 95, loss = 0.36303725\n",
      "Iteration 96, loss = 0.36120037\n",
      "Iteration 97, loss = 0.35982335\n",
      "Iteration 98, loss = 0.36017555\n",
      "Iteration 99, loss = 0.35900524\n",
      "Iteration 100, loss = 0.36022262\n",
      "Iteration 101, loss = 0.35678805\n",
      "Iteration 102, loss = 0.35849587\n",
      "Iteration 103, loss = 0.35545120\n",
      "Iteration 104, loss = 0.35593488\n",
      "Iteration 105, loss = 0.35304407\n",
      "Iteration 106, loss = 0.35146433\n",
      "Iteration 107, loss = 0.35433559\n",
      "Iteration 108, loss = 0.35300147\n",
      "Iteration 109, loss = 0.35084403\n",
      "Iteration 110, loss = 0.35020485\n",
      "Iteration 111, loss = 0.34968731\n",
      "Iteration 112, loss = 0.35141491\n",
      "Iteration 113, loss = 0.34968591\n",
      "Iteration 114, loss = 0.34856589\n",
      "Iteration 115, loss = 0.34844378\n",
      "Iteration 116, loss = 0.34661007\n",
      "Iteration 117, loss = 0.34512180\n",
      "Iteration 118, loss = 0.34626063\n",
      "Iteration 119, loss = 0.34670672\n",
      "Iteration 120, loss = 0.34570929\n",
      "Iteration 121, loss = 0.34490538\n",
      "Iteration 122, loss = 0.34279874\n",
      "Iteration 123, loss = 0.34256732\n",
      "Iteration 124, loss = 0.34209839\n",
      "Iteration 125, loss = 0.34082101\n",
      "Iteration 126, loss = 0.34233130\n",
      "Iteration 127, loss = 0.33923089\n",
      "Iteration 128, loss = 0.34218144\n",
      "Iteration 129, loss = 0.33813995\n",
      "Iteration 130, loss = 0.34135549\n",
      "Iteration 131, loss = 0.33816552\n",
      "Iteration 132, loss = 0.33870414\n",
      "Iteration 133, loss = 0.33744199\n",
      "Iteration 134, loss = 0.33734968\n",
      "Iteration 135, loss = 0.33607424\n",
      "Iteration 136, loss = 0.33318417\n",
      "Iteration 137, loss = 0.33409593\n",
      "Iteration 138, loss = 0.33451719\n",
      "Iteration 139, loss = 0.33433270\n",
      "Iteration 140, loss = 0.33224980\n",
      "Iteration 141, loss = 0.33240918\n",
      "Iteration 142, loss = 0.33135785\n",
      "Iteration 143, loss = 0.33267617\n",
      "Iteration 144, loss = 0.33119900\n",
      "Iteration 145, loss = 0.32942333\n",
      "Iteration 146, loss = 0.32852467\n",
      "Iteration 147, loss = 0.32905179\n",
      "Iteration 148, loss = 0.32846543\n",
      "Iteration 149, loss = 0.32905990\n",
      "Iteration 150, loss = 0.32747206\n",
      "Iteration 151, loss = 0.32529364\n",
      "Iteration 152, loss = 0.32567946\n",
      "Iteration 153, loss = 0.32628433\n",
      "Iteration 154, loss = 0.32521773\n",
      "Iteration 155, loss = 0.32389040\n",
      "Iteration 156, loss = 0.32640024\n",
      "Iteration 157, loss = 0.32730668\n",
      "Iteration 158, loss = 0.32495218\n",
      "Iteration 159, loss = 0.32304975\n",
      "Iteration 160, loss = 0.32314189\n",
      "Iteration 161, loss = 0.32154371\n",
      "Iteration 162, loss = 0.31966063\n",
      "Iteration 163, loss = 0.32384172\n",
      "Iteration 164, loss = 0.32137540\n",
      "Iteration 165, loss = 0.31814318\n",
      "Iteration 166, loss = 0.32042730\n",
      "Iteration 167, loss = 0.31761553\n",
      "Iteration 168, loss = 0.31772470\n",
      "Iteration 169, loss = 0.31671238\n",
      "Iteration 170, loss = 0.32100178\n",
      "Iteration 171, loss = 0.31702853\n",
      "Iteration 172, loss = 0.32004073\n",
      "Iteration 173, loss = 0.31712458\n",
      "Iteration 174, loss = 0.31438858\n",
      "Iteration 175, loss = 0.31594353\n",
      "Iteration 176, loss = 0.31533777\n",
      "Iteration 177, loss = 0.31556324\n",
      "Iteration 178, loss = 0.31343794\n",
      "Iteration 179, loss = 0.31264164\n",
      "Iteration 180, loss = 0.31367944\n",
      "Iteration 181, loss = 0.31254435\n",
      "Iteration 182, loss = 0.31192514\n",
      "Iteration 183, loss = 0.31229403\n",
      "Iteration 184, loss = 0.31226373\n",
      "Iteration 185, loss = 0.30970066\n",
      "Iteration 186, loss = 0.31072937\n",
      "Iteration 187, loss = 0.30897996\n",
      "Iteration 188, loss = 0.30911382\n",
      "Iteration 189, loss = 0.30939559\n",
      "Iteration 190, loss = 0.30773838\n",
      "Iteration 191, loss = 0.31145449\n",
      "Iteration 192, loss = 0.30726877\n",
      "Iteration 193, loss = 0.30654278\n",
      "Iteration 194, loss = 0.30776512\n",
      "Iteration 195, loss = 0.30679536\n",
      "Iteration 196, loss = 0.30663752\n",
      "Iteration 197, loss = 0.30441822\n",
      "Iteration 198, loss = 0.30444039\n",
      "Iteration 199, loss = 0.30400476\n",
      "Iteration 200, loss = 0.30435823\n",
      "Iteration 1, loss = 0.68093825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.53847265\n",
      "Iteration 3, loss = 0.48973046\n",
      "Iteration 4, loss = 0.48471477\n",
      "Iteration 5, loss = 0.48252780\n",
      "Iteration 6, loss = 0.48270285\n",
      "Iteration 7, loss = 0.48009917\n",
      "Iteration 8, loss = 0.47834907\n",
      "Iteration 9, loss = 0.47667048\n",
      "Iteration 10, loss = 0.47677088\n",
      "Iteration 11, loss = 0.47539965\n",
      "Iteration 12, loss = 0.47499037\n",
      "Iteration 13, loss = 0.47369321\n",
      "Iteration 14, loss = 0.47122642\n",
      "Iteration 15, loss = 0.47113589\n",
      "Iteration 16, loss = 0.46847126\n",
      "Iteration 17, loss = 0.46868349\n",
      "Iteration 18, loss = 0.46585358\n",
      "Iteration 19, loss = 0.46392040\n",
      "Iteration 20, loss = 0.46147280\n",
      "Iteration 21, loss = 0.45981891\n",
      "Iteration 22, loss = 0.46010528\n",
      "Iteration 23, loss = 0.45600082\n",
      "Iteration 24, loss = 0.45292542\n",
      "Iteration 25, loss = 0.45511238\n",
      "Iteration 26, loss = 0.45072206\n",
      "Iteration 27, loss = 0.44775862\n",
      "Iteration 28, loss = 0.44698339\n",
      "Iteration 29, loss = 0.44253964\n",
      "Iteration 30, loss = 0.44233319\n",
      "Iteration 31, loss = 0.43860348\n",
      "Iteration 32, loss = 0.43576539\n",
      "Iteration 33, loss = 0.43427118\n",
      "Iteration 34, loss = 0.43348136\n",
      "Iteration 35, loss = 0.43093226\n",
      "Iteration 36, loss = 0.42668958\n",
      "Iteration 37, loss = 0.42503197\n",
      "Iteration 38, loss = 0.42258336\n",
      "Iteration 39, loss = 0.42164201\n",
      "Iteration 40, loss = 0.42146531\n",
      "Iteration 41, loss = 0.41900019\n",
      "Iteration 42, loss = 0.42001759\n",
      "Iteration 43, loss = 0.41406854\n",
      "Iteration 44, loss = 0.41435660\n",
      "Iteration 45, loss = 0.41262817\n",
      "Iteration 46, loss = 0.41426800\n",
      "Iteration 47, loss = 0.40998016\n",
      "Iteration 48, loss = 0.41102590\n",
      "Iteration 49, loss = 0.40563146\n",
      "Iteration 50, loss = 0.40732184\n",
      "Iteration 51, loss = 0.40348827\n",
      "Iteration 52, loss = 0.40481818\n",
      "Iteration 53, loss = 0.40258377\n",
      "Iteration 54, loss = 0.39929734\n",
      "Iteration 55, loss = 0.40058747\n",
      "Iteration 56, loss = 0.39604477\n",
      "Iteration 57, loss = 0.39599442\n",
      "Iteration 58, loss = 0.39744764\n",
      "Iteration 59, loss = 0.39545117\n",
      "Iteration 60, loss = 0.39574431\n",
      "Iteration 61, loss = 0.39420082\n",
      "Iteration 62, loss = 0.39196346\n",
      "Iteration 63, loss = 0.39190733\n",
      "Iteration 64, loss = 0.39128639\n",
      "Iteration 65, loss = 0.38970979\n",
      "Iteration 66, loss = 0.38808447\n",
      "Iteration 67, loss = 0.38935754\n",
      "Iteration 68, loss = 0.38540689\n",
      "Iteration 69, loss = 0.38557254\n",
      "Iteration 70, loss = 0.38396300\n",
      "Iteration 71, loss = 0.38340896\n",
      "Iteration 72, loss = 0.38221949\n",
      "Iteration 73, loss = 0.37948515\n",
      "Iteration 74, loss = 0.38088143\n",
      "Iteration 75, loss = 0.37783017\n",
      "Iteration 76, loss = 0.37948818\n",
      "Iteration 77, loss = 0.37782844\n",
      "Iteration 78, loss = 0.37541190\n",
      "Iteration 79, loss = 0.37536601\n",
      "Iteration 80, loss = 0.37735743\n",
      "Iteration 81, loss = 0.37378949\n",
      "Iteration 82, loss = 0.37354539\n",
      "Iteration 83, loss = 0.37289656\n",
      "Iteration 84, loss = 0.37186461\n",
      "Iteration 85, loss = 0.37231683\n",
      "Iteration 86, loss = 0.36984620\n",
      "Iteration 87, loss = 0.37032487\n",
      "Iteration 88, loss = 0.36953998\n",
      "Iteration 89, loss = 0.36947705\n",
      "Iteration 90, loss = 0.36831542\n",
      "Iteration 91, loss = 0.36631475\n",
      "Iteration 92, loss = 0.36699686\n",
      "Iteration 93, loss = 0.36603747\n",
      "Iteration 94, loss = 0.36439125\n",
      "Iteration 95, loss = 0.36308570\n",
      "Iteration 96, loss = 0.36330208\n",
      "Iteration 97, loss = 0.36104719\n",
      "Iteration 98, loss = 0.36352537\n",
      "Iteration 99, loss = 0.36060012\n",
      "Iteration 100, loss = 0.36179613\n",
      "Iteration 101, loss = 0.35908539\n",
      "Iteration 102, loss = 0.35971368\n",
      "Iteration 103, loss = 0.35718852\n",
      "Iteration 104, loss = 0.35740786\n",
      "Iteration 105, loss = 0.35678482\n",
      "Iteration 106, loss = 0.35652099\n",
      "Iteration 107, loss = 0.35403300\n",
      "Iteration 108, loss = 0.35212201\n",
      "Iteration 109, loss = 0.35321807\n",
      "Iteration 110, loss = 0.35251879\n",
      "Iteration 111, loss = 0.35058926\n",
      "Iteration 112, loss = 0.35509841\n",
      "Iteration 113, loss = 0.34883754\n",
      "Iteration 114, loss = 0.34965643\n",
      "Iteration 115, loss = 0.34986522\n",
      "Iteration 116, loss = 0.34857921\n",
      "Iteration 117, loss = 0.34848300\n",
      "Iteration 118, loss = 0.34637689\n",
      "Iteration 119, loss = 0.34671945\n",
      "Iteration 120, loss = 0.34600268\n",
      "Iteration 121, loss = 0.34640553\n",
      "Iteration 122, loss = 0.34398204\n",
      "Iteration 123, loss = 0.34380343\n",
      "Iteration 124, loss = 0.34263121\n",
      "Iteration 125, loss = 0.34512879\n",
      "Iteration 126, loss = 0.34131767\n",
      "Iteration 127, loss = 0.34299665\n",
      "Iteration 128, loss = 0.34266745\n",
      "Iteration 129, loss = 0.34304799\n",
      "Iteration 130, loss = 0.33991094\n",
      "Iteration 131, loss = 0.34243658\n",
      "Iteration 132, loss = 0.33916301\n",
      "Iteration 133, loss = 0.33807872\n",
      "Iteration 134, loss = 0.33758093\n",
      "Iteration 135, loss = 0.33664334\n",
      "Iteration 136, loss = 0.33945529\n",
      "Iteration 137, loss = 0.33640456\n",
      "Iteration 138, loss = 0.33442258\n",
      "Iteration 139, loss = 0.33426481\n",
      "Iteration 140, loss = 0.33354685\n",
      "Iteration 141, loss = 0.33299150\n",
      "Iteration 142, loss = 0.33566213\n",
      "Iteration 143, loss = 0.33251554\n",
      "Iteration 144, loss = 0.33152540\n",
      "Iteration 145, loss = 0.32949573\n",
      "Iteration 146, loss = 0.32906248\n",
      "Iteration 147, loss = 0.33137928\n",
      "Iteration 148, loss = 0.32986438\n",
      "Iteration 149, loss = 0.33137221\n",
      "Iteration 150, loss = 0.32755954\n",
      "Iteration 151, loss = 0.33048044\n",
      "Iteration 152, loss = 0.32938813\n",
      "Iteration 153, loss = 0.32961524\n",
      "Iteration 154, loss = 0.32630819\n",
      "Iteration 155, loss = 0.32720222\n",
      "Iteration 156, loss = 0.32692357\n",
      "Iteration 157, loss = 0.32500410\n",
      "Iteration 158, loss = 0.32524354\n",
      "Iteration 159, loss = 0.32268078\n",
      "Iteration 160, loss = 0.32513846\n",
      "Iteration 161, loss = 0.32528734\n",
      "Iteration 162, loss = 0.32298631\n",
      "Iteration 163, loss = 0.32127299\n",
      "Iteration 164, loss = 0.32114565\n",
      "Iteration 165, loss = 0.31981163\n",
      "Iteration 166, loss = 0.32195963\n",
      "Iteration 167, loss = 0.32022427\n",
      "Iteration 168, loss = 0.31945641\n",
      "Iteration 169, loss = 0.31799561\n",
      "Iteration 170, loss = 0.31636408\n",
      "Iteration 171, loss = 0.31853596\n",
      "Iteration 172, loss = 0.31758425\n",
      "Iteration 173, loss = 0.31770154\n",
      "Iteration 174, loss = 0.32124381\n",
      "Iteration 175, loss = 0.31547438\n",
      "Iteration 176, loss = 0.31471123\n",
      "Iteration 177, loss = 0.31413528\n",
      "Iteration 178, loss = 0.31342676\n",
      "Iteration 179, loss = 0.31497413\n",
      "Iteration 180, loss = 0.31322332\n",
      "Iteration 181, loss = 0.31156826\n",
      "Iteration 182, loss = 0.31208966\n",
      "Iteration 183, loss = 0.31175363\n",
      "Iteration 184, loss = 0.30943779\n",
      "Iteration 185, loss = 0.31038494\n",
      "Iteration 186, loss = 0.30957809\n",
      "Iteration 187, loss = 0.30964958\n",
      "Iteration 188, loss = 0.30983210\n",
      "Iteration 189, loss = 0.30800206\n",
      "Iteration 190, loss = 0.30756155\n",
      "Iteration 191, loss = 0.30864739\n",
      "Iteration 192, loss = 0.30606795\n",
      "Iteration 193, loss = 0.30556034\n",
      "Iteration 194, loss = 0.30407671\n",
      "Iteration 195, loss = 0.30547767\n",
      "Iteration 196, loss = 0.30367047\n",
      "Iteration 197, loss = 0.30252699\n",
      "Iteration 198, loss = 0.30202844\n",
      "Iteration 199, loss = 0.30408762\n",
      "Iteration 200, loss = 0.30342477\n",
      "Iteration 1, loss = 0.69480055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.57261450\n",
      "Iteration 3, loss = 0.49419006\n",
      "Iteration 4, loss = 0.48855317\n",
      "Iteration 5, loss = 0.48405908\n",
      "Iteration 6, loss = 0.48496349\n",
      "Iteration 7, loss = 0.48213920\n",
      "Iteration 8, loss = 0.48500867\n",
      "Iteration 9, loss = 0.47932667\n",
      "Iteration 10, loss = 0.47883223\n",
      "Iteration 11, loss = 0.47734215\n",
      "Iteration 12, loss = 0.47807548\n",
      "Iteration 13, loss = 0.47448907\n",
      "Iteration 14, loss = 0.47392773\n",
      "Iteration 15, loss = 0.47444546\n",
      "Iteration 16, loss = 0.47254568\n",
      "Iteration 17, loss = 0.47199460\n",
      "Iteration 18, loss = 0.47074674\n",
      "Iteration 19, loss = 0.46865807\n",
      "Iteration 20, loss = 0.46745521\n",
      "Iteration 21, loss = 0.46808785\n",
      "Iteration 22, loss = 0.46650721\n",
      "Iteration 23, loss = 0.46470517\n",
      "Iteration 24, loss = 0.46352284\n",
      "Iteration 25, loss = 0.46161286\n",
      "Iteration 26, loss = 0.46036215\n",
      "Iteration 27, loss = 0.46014267\n",
      "Iteration 28, loss = 0.45792562\n",
      "Iteration 29, loss = 0.45827859\n",
      "Iteration 30, loss = 0.45381645\n",
      "Iteration 31, loss = 0.45671742\n",
      "Iteration 32, loss = 0.45222958\n",
      "Iteration 33, loss = 0.45146026\n",
      "Iteration 34, loss = 0.44927330\n",
      "Iteration 35, loss = 0.44719435\n",
      "Iteration 36, loss = 0.44576479\n",
      "Iteration 37, loss = 0.44482281\n",
      "Iteration 38, loss = 0.44331576\n",
      "Iteration 39, loss = 0.44349294\n",
      "Iteration 40, loss = 0.44032198\n",
      "Iteration 41, loss = 0.43969377\n",
      "Iteration 42, loss = 0.43793043\n",
      "Iteration 43, loss = 0.43386212\n",
      "Iteration 44, loss = 0.43364387\n",
      "Iteration 45, loss = 0.43101286\n",
      "Iteration 46, loss = 0.42835833\n",
      "Iteration 47, loss = 0.42700419\n",
      "Iteration 48, loss = 0.42552851\n",
      "Iteration 49, loss = 0.42357175\n",
      "Iteration 50, loss = 0.42335535\n",
      "Iteration 51, loss = 0.41931221\n",
      "Iteration 52, loss = 0.41806160\n",
      "Iteration 53, loss = 0.41647741\n",
      "Iteration 54, loss = 0.41710534\n",
      "Iteration 55, loss = 0.41276093\n",
      "Iteration 56, loss = 0.41523916\n",
      "Iteration 57, loss = 0.41434976\n",
      "Iteration 58, loss = 0.41011960\n",
      "Iteration 59, loss = 0.40944639\n",
      "Iteration 60, loss = 0.40940679\n",
      "Iteration 61, loss = 0.40873506\n",
      "Iteration 62, loss = 0.40930427\n",
      "Iteration 63, loss = 0.40350229\n",
      "Iteration 64, loss = 0.40608944\n",
      "Iteration 65, loss = 0.40434448\n",
      "Iteration 66, loss = 0.40571396\n",
      "Iteration 67, loss = 0.40472198\n",
      "Iteration 68, loss = 0.40252345\n",
      "Iteration 69, loss = 0.40224143\n",
      "Iteration 70, loss = 0.40050433\n",
      "Iteration 71, loss = 0.39919587\n",
      "Iteration 72, loss = 0.39962476\n",
      "Iteration 73, loss = 0.39956688\n",
      "Iteration 74, loss = 0.39696883\n",
      "Iteration 75, loss = 0.39650032\n",
      "Iteration 76, loss = 0.39771561\n",
      "Iteration 77, loss = 0.39568040\n",
      "Iteration 78, loss = 0.39806097\n",
      "Iteration 79, loss = 0.39393273\n",
      "Iteration 80, loss = 0.39381851\n",
      "Iteration 81, loss = 0.39315016\n",
      "Iteration 82, loss = 0.39234462\n",
      "Iteration 83, loss = 0.39053240\n",
      "Iteration 84, loss = 0.39002841\n",
      "Iteration 85, loss = 0.39267647\n",
      "Iteration 86, loss = 0.39034695\n",
      "Iteration 87, loss = 0.38665274\n",
      "Iteration 88, loss = 0.38724668\n",
      "Iteration 89, loss = 0.39119776\n",
      "Iteration 90, loss = 0.38682952\n",
      "Iteration 91, loss = 0.38532686\n",
      "Iteration 92, loss = 0.38478170\n",
      "Iteration 93, loss = 0.38688532\n",
      "Iteration 94, loss = 0.38754611\n",
      "Iteration 95, loss = 0.38404024\n",
      "Iteration 96, loss = 0.38328139\n",
      "Iteration 97, loss = 0.38014238\n",
      "Iteration 98, loss = 0.38172370\n",
      "Iteration 99, loss = 0.38132590\n",
      "Iteration 100, loss = 0.37953354\n",
      "Iteration 101, loss = 0.38248877\n",
      "Iteration 102, loss = 0.38122478\n",
      "Iteration 103, loss = 0.37728443\n",
      "Iteration 104, loss = 0.37822300\n",
      "Iteration 105, loss = 0.37852437\n",
      "Iteration 106, loss = 0.37790958\n",
      "Iteration 107, loss = 0.37497066\n",
      "Iteration 108, loss = 0.37500826\n",
      "Iteration 109, loss = 0.37317844\n",
      "Iteration 110, loss = 0.37621386\n",
      "Iteration 111, loss = 0.37523279\n",
      "Iteration 112, loss = 0.37204751\n",
      "Iteration 113, loss = 0.37210546\n",
      "Iteration 114, loss = 0.37242226\n",
      "Iteration 115, loss = 0.37043262\n",
      "Iteration 116, loss = 0.37059331\n",
      "Iteration 117, loss = 0.37115058\n",
      "Iteration 118, loss = 0.36946397\n",
      "Iteration 119, loss = 0.37121606\n",
      "Iteration 120, loss = 0.36941689\n",
      "Iteration 121, loss = 0.36666941\n",
      "Iteration 122, loss = 0.36505201\n",
      "Iteration 123, loss = 0.36810660\n",
      "Iteration 124, loss = 0.36464500\n",
      "Iteration 125, loss = 0.36519861\n",
      "Iteration 126, loss = 0.36245870\n",
      "Iteration 127, loss = 0.36678318\n",
      "Iteration 128, loss = 0.36242263\n",
      "Iteration 129, loss = 0.36330004\n",
      "Iteration 130, loss = 0.36052509\n",
      "Iteration 131, loss = 0.35816593\n",
      "Iteration 132, loss = 0.35853068\n",
      "Iteration 133, loss = 0.35971142\n",
      "Iteration 134, loss = 0.35974274\n",
      "Iteration 135, loss = 0.35611961\n",
      "Iteration 136, loss = 0.35456609\n",
      "Iteration 137, loss = 0.35346256\n",
      "Iteration 138, loss = 0.35651874\n",
      "Iteration 139, loss = 0.35165794\n",
      "Iteration 140, loss = 0.35191063\n",
      "Iteration 141, loss = 0.35414192\n",
      "Iteration 142, loss = 0.35167985\n",
      "Iteration 143, loss = 0.35242947\n",
      "Iteration 144, loss = 0.34741974\n",
      "Iteration 145, loss = 0.34940190\n",
      "Iteration 146, loss = 0.34740049\n",
      "Iteration 147, loss = 0.34731419\n",
      "Iteration 148, loss = 0.34417244\n",
      "Iteration 149, loss = 0.34618124\n",
      "Iteration 150, loss = 0.34658579\n",
      "Iteration 151, loss = 0.34410689\n",
      "Iteration 152, loss = 0.34405258\n",
      "Iteration 153, loss = 0.34322882\n",
      "Iteration 154, loss = 0.34149373\n",
      "Iteration 155, loss = 0.34355964\n",
      "Iteration 156, loss = 0.33725666\n",
      "Iteration 157, loss = 0.34016182\n",
      "Iteration 158, loss = 0.33768040\n",
      "Iteration 159, loss = 0.33555464\n",
      "Iteration 160, loss = 0.33700096\n",
      "Iteration 161, loss = 0.33803229\n",
      "Iteration 162, loss = 0.33473124\n",
      "Iteration 163, loss = 0.33231550\n",
      "Iteration 164, loss = 0.33287611\n",
      "Iteration 165, loss = 0.33215351\n",
      "Iteration 166, loss = 0.32868555\n",
      "Iteration 167, loss = 0.32801555\n",
      "Iteration 168, loss = 0.32814997\n",
      "Iteration 169, loss = 0.32977973\n",
      "Iteration 170, loss = 0.32776166\n",
      "Iteration 171, loss = 0.32653243\n",
      "Iteration 172, loss = 0.32659122\n",
      "Iteration 173, loss = 0.32231862\n",
      "Iteration 174, loss = 0.32559517\n",
      "Iteration 175, loss = 0.32142458\n",
      "Iteration 176, loss = 0.32254795\n",
      "Iteration 177, loss = 0.31907835\n",
      "Iteration 178, loss = 0.32128321\n",
      "Iteration 179, loss = 0.31784689\n",
      "Iteration 180, loss = 0.31718734\n",
      "Iteration 181, loss = 0.31502010\n",
      "Iteration 182, loss = 0.31413928\n",
      "Iteration 183, loss = 0.31572114\n",
      "Iteration 184, loss = 0.31357617\n",
      "Iteration 185, loss = 0.31151532\n",
      "Iteration 186, loss = 0.31306284\n",
      "Iteration 187, loss = 0.31138275\n",
      "Iteration 188, loss = 0.30966085\n",
      "Iteration 189, loss = 0.31099999\n",
      "Iteration 190, loss = 0.30895399\n",
      "Iteration 191, loss = 0.30816689\n",
      "Iteration 192, loss = 0.30605085\n",
      "Iteration 193, loss = 0.30770741\n",
      "Iteration 194, loss = 0.30639154\n",
      "Iteration 195, loss = 0.30499485\n",
      "Iteration 196, loss = 0.30710889\n",
      "Iteration 197, loss = 0.30506959\n",
      "Iteration 198, loss = 0.30368943\n",
      "Iteration 199, loss = 0.30224436\n",
      "Iteration 200, loss = 0.29881248\n",
      "Iteration 1, loss = 0.68760773\n",
      "Iteration 2, loss = 0.56114520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.49311577\n",
      "Iteration 4, loss = 0.48490551\n",
      "Iteration 5, loss = 0.48194700\n",
      "Iteration 6, loss = 0.48146098\n",
      "Iteration 7, loss = 0.48211971\n",
      "Iteration 8, loss = 0.47912728\n",
      "Iteration 9, loss = 0.47869409\n",
      "Iteration 10, loss = 0.47598290\n",
      "Iteration 11, loss = 0.47524109\n",
      "Iteration 12, loss = 0.47391245\n",
      "Iteration 13, loss = 0.47324406\n",
      "Iteration 14, loss = 0.47048060\n",
      "Iteration 15, loss = 0.46949981\n",
      "Iteration 16, loss = 0.46775345\n",
      "Iteration 17, loss = 0.46849546\n",
      "Iteration 18, loss = 0.46572268\n",
      "Iteration 19, loss = 0.46530880\n",
      "Iteration 20, loss = 0.46210538\n",
      "Iteration 21, loss = 0.46009152\n",
      "Iteration 22, loss = 0.45891177\n",
      "Iteration 23, loss = 0.45712481\n",
      "Iteration 24, loss = 0.45405733\n",
      "Iteration 25, loss = 0.45419642\n",
      "Iteration 26, loss = 0.45028076\n",
      "Iteration 27, loss = 0.44770813\n",
      "Iteration 28, loss = 0.44662659\n",
      "Iteration 29, loss = 0.44446001\n",
      "Iteration 30, loss = 0.44291257\n",
      "Iteration 31, loss = 0.44038972\n",
      "Iteration 32, loss = 0.43712115\n",
      "Iteration 33, loss = 0.43600917\n",
      "Iteration 34, loss = 0.43421569\n",
      "Iteration 35, loss = 0.43497454\n",
      "Iteration 36, loss = 0.42988454\n",
      "Iteration 37, loss = 0.42726826\n",
      "Iteration 38, loss = 0.42744571\n",
      "Iteration 39, loss = 0.42499945\n",
      "Iteration 40, loss = 0.42407193\n",
      "Iteration 41, loss = 0.42368958\n",
      "Iteration 42, loss = 0.41976917\n",
      "Iteration 43, loss = 0.42059417\n",
      "Iteration 44, loss = 0.41735869\n",
      "Iteration 45, loss = 0.41889916\n",
      "Iteration 46, loss = 0.41406342\n",
      "Iteration 47, loss = 0.41680752\n",
      "Iteration 48, loss = 0.41057166\n",
      "Iteration 49, loss = 0.41248310\n",
      "Iteration 50, loss = 0.40975398\n",
      "Iteration 51, loss = 0.41104864\n",
      "Iteration 52, loss = 0.40873630\n",
      "Iteration 53, loss = 0.40807381\n",
      "Iteration 54, loss = 0.40719556\n",
      "Iteration 55, loss = 0.40848071\n",
      "Iteration 56, loss = 0.40590184\n",
      "Iteration 57, loss = 0.40538424\n",
      "Iteration 58, loss = 0.40758165\n",
      "Iteration 59, loss = 0.40317046\n",
      "Iteration 60, loss = 0.40200249\n",
      "Iteration 61, loss = 0.40257556\n",
      "Iteration 62, loss = 0.39946976\n",
      "Iteration 63, loss = 0.40005876\n",
      "Iteration 64, loss = 0.39995351\n",
      "Iteration 65, loss = 0.39768629\n",
      "Iteration 66, loss = 0.39558759\n",
      "Iteration 67, loss = 0.39652997\n",
      "Iteration 68, loss = 0.39581855\n",
      "Iteration 69, loss = 0.39421778\n",
      "Iteration 70, loss = 0.39363086\n",
      "Iteration 71, loss = 0.39354840\n",
      "Iteration 72, loss = 0.39099471\n",
      "Iteration 73, loss = 0.39339057\n",
      "Iteration 74, loss = 0.38919630\n",
      "Iteration 75, loss = 0.38923372\n",
      "Iteration 76, loss = 0.38772872\n",
      "Iteration 77, loss = 0.38844122\n",
      "Iteration 78, loss = 0.38757235\n",
      "Iteration 79, loss = 0.38819681\n",
      "Iteration 80, loss = 0.38650561\n",
      "Iteration 81, loss = 0.38600731\n",
      "Iteration 82, loss = 0.38646444\n",
      "Iteration 83, loss = 0.38455964\n",
      "Iteration 84, loss = 0.38277139\n",
      "Iteration 85, loss = 0.38526805\n",
      "Iteration 86, loss = 0.37993033\n",
      "Iteration 87, loss = 0.38098597\n",
      "Iteration 88, loss = 0.38066391\n",
      "Iteration 89, loss = 0.37896194\n",
      "Iteration 90, loss = 0.38097543\n",
      "Iteration 91, loss = 0.37800978\n",
      "Iteration 92, loss = 0.37806016\n",
      "Iteration 93, loss = 0.37586537\n",
      "Iteration 94, loss = 0.37703170\n",
      "Iteration 95, loss = 0.37858692\n",
      "Iteration 96, loss = 0.37537660\n",
      "Iteration 97, loss = 0.37427870\n",
      "Iteration 98, loss = 0.37335945\n",
      "Iteration 99, loss = 0.37453139\n",
      "Iteration 100, loss = 0.37332091\n",
      "Iteration 101, loss = 0.37264242\n",
      "Iteration 102, loss = 0.37195286\n",
      "Iteration 103, loss = 0.37229848\n",
      "Iteration 104, loss = 0.37033746\n",
      "Iteration 105, loss = 0.36880601\n",
      "Iteration 106, loss = 0.36909580\n",
      "Iteration 107, loss = 0.36729592\n",
      "Iteration 108, loss = 0.36846701\n",
      "Iteration 109, loss = 0.36789252\n",
      "Iteration 110, loss = 0.36781051\n",
      "Iteration 111, loss = 0.36690948\n",
      "Iteration 112, loss = 0.36801692\n",
      "Iteration 113, loss = 0.36425718\n",
      "Iteration 114, loss = 0.36406343\n",
      "Iteration 115, loss = 0.36428111\n",
      "Iteration 116, loss = 0.36319750\n",
      "Iteration 117, loss = 0.36183042\n",
      "Iteration 118, loss = 0.36253153\n",
      "Iteration 119, loss = 0.36114128\n",
      "Iteration 120, loss = 0.35977012\n",
      "Iteration 121, loss = 0.35987364\n",
      "Iteration 122, loss = 0.35822836\n",
      "Iteration 123, loss = 0.35759149\n",
      "Iteration 124, loss = 0.35640803\n",
      "Iteration 125, loss = 0.35661659\n",
      "Iteration 126, loss = 0.35622896\n",
      "Iteration 127, loss = 0.35434714\n",
      "Iteration 128, loss = 0.35584397\n",
      "Iteration 129, loss = 0.35626494\n",
      "Iteration 130, loss = 0.35433797\n",
      "Iteration 131, loss = 0.35476196\n",
      "Iteration 132, loss = 0.35282152\n",
      "Iteration 133, loss = 0.35364413\n",
      "Iteration 134, loss = 0.35022888\n",
      "Iteration 135, loss = 0.35122587\n",
      "Iteration 136, loss = 0.34989341\n",
      "Iteration 137, loss = 0.34949936\n",
      "Iteration 138, loss = 0.34857900\n",
      "Iteration 139, loss = 0.34946894\n",
      "Iteration 140, loss = 0.34787456\n",
      "Iteration 141, loss = 0.34714651\n",
      "Iteration 142, loss = 0.34577017\n",
      "Iteration 143, loss = 0.34673704\n",
      "Iteration 144, loss = 0.34571232\n",
      "Iteration 145, loss = 0.34270672\n",
      "Iteration 146, loss = 0.34363484\n",
      "Iteration 147, loss = 0.34110322\n",
      "Iteration 148, loss = 0.34198503\n",
      "Iteration 149, loss = 0.34139876\n",
      "Iteration 150, loss = 0.34013558\n",
      "Iteration 151, loss = 0.34133428\n",
      "Iteration 152, loss = 0.33781652\n",
      "Iteration 153, loss = 0.33654218\n",
      "Iteration 154, loss = 0.33838692\n",
      "Iteration 155, loss = 0.33910107\n",
      "Iteration 156, loss = 0.33485030\n",
      "Iteration 157, loss = 0.33541635\n",
      "Iteration 158, loss = 0.33484385\n",
      "Iteration 159, loss = 0.33420270\n",
      "Iteration 160, loss = 0.33209793\n",
      "Iteration 161, loss = 0.33047166\n",
      "Iteration 162, loss = 0.33201339\n",
      "Iteration 163, loss = 0.33081451\n",
      "Iteration 164, loss = 0.32960486\n",
      "Iteration 165, loss = 0.33313221\n",
      "Iteration 166, loss = 0.33011720\n",
      "Iteration 167, loss = 0.32641093\n",
      "Iteration 168, loss = 0.32757610\n",
      "Iteration 169, loss = 0.32840467\n",
      "Iteration 170, loss = 0.32736407\n",
      "Iteration 171, loss = 0.32660133\n",
      "Iteration 172, loss = 0.32444964\n",
      "Iteration 173, loss = 0.32203876\n",
      "Iteration 174, loss = 0.32158131\n",
      "Iteration 175, loss = 0.32249666\n",
      "Iteration 176, loss = 0.32026401\n",
      "Iteration 177, loss = 0.32035719\n",
      "Iteration 178, loss = 0.32034153\n",
      "Iteration 179, loss = 0.31912702\n",
      "Iteration 180, loss = 0.31880603\n",
      "Iteration 181, loss = 0.31399600\n",
      "Iteration 182, loss = 0.31444136\n",
      "Iteration 183, loss = 0.31618603\n",
      "Iteration 184, loss = 0.31635172\n",
      "Iteration 185, loss = 0.31318105\n",
      "Iteration 186, loss = 0.31118724\n",
      "Iteration 187, loss = 0.31280500\n",
      "Iteration 188, loss = 0.31075358\n",
      "Iteration 189, loss = 0.31220491\n",
      "Iteration 190, loss = 0.30969746\n",
      "Iteration 191, loss = 0.30868471\n",
      "Iteration 192, loss = 0.30857561\n",
      "Iteration 193, loss = 0.30704482\n",
      "Iteration 194, loss = 0.30595501\n",
      "Iteration 195, loss = 0.30473348\n",
      "Iteration 196, loss = 0.30493900\n",
      "Iteration 197, loss = 0.30505826\n",
      "Iteration 198, loss = 0.30346235\n",
      "Iteration 199, loss = 0.30220731\n",
      "Iteration 200, loss = 0.30513823\n",
      "Iteration 1, loss = 0.69595163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69712385\n",
      "Iteration 3, loss = 0.69516245\n",
      "Iteration 4, loss = 0.68244304\n",
      "Iteration 5, loss = 0.58809184\n",
      "Iteration 6, loss = 0.49833284\n",
      "Iteration 7, loss = 0.48523158\n",
      "Iteration 8, loss = 0.48413539\n",
      "Iteration 9, loss = 0.48220034\n",
      "Iteration 10, loss = 0.48256135\n",
      "Iteration 11, loss = 0.48018533\n",
      "Iteration 12, loss = 0.47861823\n",
      "Iteration 13, loss = 0.47918365\n",
      "Iteration 14, loss = 0.47757100\n",
      "Iteration 15, loss = 0.47664862\n",
      "Iteration 16, loss = 0.47646470\n",
      "Iteration 17, loss = 0.47550035\n",
      "Iteration 18, loss = 0.47541762\n",
      "Iteration 19, loss = 0.47274691\n",
      "Iteration 20, loss = 0.47437459\n",
      "Iteration 21, loss = 0.47122221\n",
      "Iteration 22, loss = 0.47255104\n",
      "Iteration 23, loss = 0.47018580\n",
      "Iteration 24, loss = 0.46770978\n",
      "Iteration 25, loss = 0.46866150\n",
      "Iteration 26, loss = 0.46663788\n",
      "Iteration 27, loss = 0.46651453\n",
      "Iteration 28, loss = 0.46449171\n",
      "Iteration 29, loss = 0.46451628\n",
      "Iteration 30, loss = 0.46274926\n",
      "Iteration 31, loss = 0.46006309\n",
      "Iteration 32, loss = 0.46022017\n",
      "Iteration 33, loss = 0.45765836\n",
      "Iteration 34, loss = 0.45740084\n",
      "Iteration 35, loss = 0.45369981\n",
      "Iteration 36, loss = 0.45342957\n",
      "Iteration 37, loss = 0.45100072\n",
      "Iteration 38, loss = 0.45007048\n",
      "Iteration 39, loss = 0.44880549\n",
      "Iteration 40, loss = 0.44716692\n",
      "Iteration 41, loss = 0.44494563\n",
      "Iteration 42, loss = 0.44542512\n",
      "Iteration 43, loss = 0.44314155\n",
      "Iteration 44, loss = 0.43977222\n",
      "Iteration 45, loss = 0.43948486\n",
      "Iteration 46, loss = 0.43721199\n",
      "Iteration 47, loss = 0.43493482\n",
      "Iteration 48, loss = 0.43346784\n",
      "Iteration 49, loss = 0.43081651\n",
      "Iteration 50, loss = 0.43142932\n",
      "Iteration 51, loss = 0.42811315\n",
      "Iteration 52, loss = 0.42752787\n",
      "Iteration 53, loss = 0.42593277\n",
      "Iteration 54, loss = 0.42363143\n",
      "Iteration 55, loss = 0.41956303\n",
      "Iteration 56, loss = 0.41938652\n",
      "Iteration 57, loss = 0.42081386\n",
      "Iteration 58, loss = 0.41861366\n",
      "Iteration 59, loss = 0.41515393\n",
      "Iteration 60, loss = 0.41160343\n",
      "Iteration 61, loss = 0.41103337\n",
      "Iteration 62, loss = 0.40859255\n",
      "Iteration 63, loss = 0.41337624\n",
      "Iteration 64, loss = 0.41045381\n",
      "Iteration 65, loss = 0.40664895\n",
      "Iteration 66, loss = 0.40938985\n",
      "Iteration 67, loss = 0.40830170\n",
      "Iteration 68, loss = 0.40612378\n",
      "Iteration 69, loss = 0.40260808\n",
      "Iteration 70, loss = 0.40281798\n",
      "Iteration 71, loss = 0.40115369\n",
      "Iteration 72, loss = 0.40198464\n",
      "Iteration 73, loss = 0.39764632\n",
      "Iteration 74, loss = 0.39891714\n",
      "Iteration 75, loss = 0.39743023\n",
      "Iteration 76, loss = 0.39741830\n",
      "Iteration 77, loss = 0.39553953\n",
      "Iteration 78, loss = 0.39590179\n",
      "Iteration 79, loss = 0.39445842\n",
      "Iteration 80, loss = 0.39532634\n",
      "Iteration 81, loss = 0.39251080\n",
      "Iteration 82, loss = 0.39219007\n",
      "Iteration 83, loss = 0.39131872\n",
      "Iteration 84, loss = 0.39065835\n",
      "Iteration 85, loss = 0.39069999\n",
      "Iteration 86, loss = 0.38837905\n",
      "Iteration 87, loss = 0.39035233\n",
      "Iteration 88, loss = 0.38927565\n",
      "Iteration 89, loss = 0.38629374\n",
      "Iteration 90, loss = 0.38659682\n",
      "Iteration 91, loss = 0.38443406\n",
      "Iteration 92, loss = 0.38747063\n",
      "Iteration 93, loss = 0.38346921\n",
      "Iteration 94, loss = 0.38292262\n",
      "Iteration 95, loss = 0.38304445\n",
      "Iteration 96, loss = 0.38018257\n",
      "Iteration 97, loss = 0.38060171\n",
      "Iteration 98, loss = 0.37886505\n",
      "Iteration 99, loss = 0.37752442\n",
      "Iteration 100, loss = 0.37674845\n",
      "Iteration 101, loss = 0.37509921\n",
      "Iteration 102, loss = 0.37779871\n",
      "Iteration 103, loss = 0.37414835\n",
      "Iteration 104, loss = 0.37627906\n",
      "Iteration 105, loss = 0.37320251\n",
      "Iteration 106, loss = 0.37185726\n",
      "Iteration 107, loss = 0.37342964\n",
      "Iteration 108, loss = 0.37398878\n",
      "Iteration 109, loss = 0.37154375\n",
      "Iteration 110, loss = 0.36977305\n",
      "Iteration 111, loss = 0.36909203\n",
      "Iteration 112, loss = 0.37263845\n",
      "Iteration 113, loss = 0.36875147\n",
      "Iteration 114, loss = 0.36733927\n",
      "Iteration 115, loss = 0.36724369\n",
      "Iteration 116, loss = 0.36455083\n",
      "Iteration 117, loss = 0.36357073\n",
      "Iteration 118, loss = 0.36681897\n",
      "Iteration 119, loss = 0.36465178\n",
      "Iteration 120, loss = 0.36187157\n",
      "Iteration 121, loss = 0.36274639\n",
      "Iteration 122, loss = 0.36234081\n",
      "Iteration 123, loss = 0.36064120\n",
      "Iteration 124, loss = 0.36156079\n",
      "Iteration 125, loss = 0.36239071\n",
      "Iteration 126, loss = 0.35835875\n",
      "Iteration 127, loss = 0.35971158\n",
      "Iteration 128, loss = 0.35930698\n",
      "Iteration 129, loss = 0.35409134\n",
      "Iteration 130, loss = 0.35554959\n",
      "Iteration 131, loss = 0.35541242\n",
      "Iteration 132, loss = 0.35305279\n",
      "Iteration 133, loss = 0.35571145\n",
      "Iteration 134, loss = 0.35273155\n",
      "Iteration 135, loss = 0.35042971\n",
      "Iteration 136, loss = 0.35367829\n",
      "Iteration 137, loss = 0.35352293\n",
      "Iteration 138, loss = 0.34929691\n",
      "Iteration 139, loss = 0.34885033\n",
      "Iteration 140, loss = 0.34878461\n",
      "Iteration 141, loss = 0.34798298\n",
      "Iteration 142, loss = 0.34866610\n",
      "Iteration 143, loss = 0.34618672\n",
      "Iteration 144, loss = 0.34903314\n",
      "Iteration 145, loss = 0.34423577\n",
      "Iteration 146, loss = 0.34493064\n",
      "Iteration 147, loss = 0.34475231\n",
      "Iteration 148, loss = 0.34146557\n",
      "Iteration 149, loss = 0.34329409\n",
      "Iteration 150, loss = 0.34175977\n",
      "Iteration 151, loss = 0.34202328\n",
      "Iteration 152, loss = 0.34183269\n",
      "Iteration 153, loss = 0.33893258\n",
      "Iteration 154, loss = 0.34030392\n",
      "Iteration 155, loss = 0.33863584\n",
      "Iteration 156, loss = 0.34079780\n",
      "Iteration 157, loss = 0.34169662\n",
      "Iteration 158, loss = 0.33841482\n",
      "Iteration 159, loss = 0.33703280\n",
      "Iteration 160, loss = 0.33990872\n",
      "Iteration 161, loss = 0.33610422\n",
      "Iteration 162, loss = 0.33443527\n",
      "Iteration 163, loss = 0.33500684\n",
      "Iteration 164, loss = 0.33513134\n",
      "Iteration 165, loss = 0.33458705\n",
      "Iteration 166, loss = 0.33397814\n",
      "Iteration 167, loss = 0.33393313\n",
      "Iteration 168, loss = 0.33052750\n",
      "Iteration 169, loss = 0.33472676\n",
      "Iteration 170, loss = 0.33033483\n",
      "Iteration 171, loss = 0.32963187\n",
      "Iteration 172, loss = 0.32800470\n",
      "Iteration 173, loss = 0.33037050\n",
      "Iteration 174, loss = 0.33078820\n",
      "Iteration 175, loss = 0.33321157\n",
      "Iteration 176, loss = 0.32625763\n",
      "Iteration 177, loss = 0.32631570\n",
      "Iteration 178, loss = 0.32887626\n",
      "Iteration 179, loss = 0.32690600\n",
      "Iteration 180, loss = 0.32609498\n",
      "Iteration 181, loss = 0.32479913\n",
      "Iteration 182, loss = 0.32419212\n",
      "Iteration 183, loss = 0.32769022\n",
      "Iteration 184, loss = 0.32241076\n",
      "Iteration 185, loss = 0.32257533\n",
      "Iteration 186, loss = 0.31966835\n",
      "Iteration 187, loss = 0.32103238\n",
      "Iteration 188, loss = 0.32418934\n",
      "Iteration 189, loss = 0.31938037\n",
      "Iteration 190, loss = 0.32249481\n",
      "Iteration 191, loss = 0.31831141\n",
      "Iteration 192, loss = 0.31860377\n",
      "Iteration 193, loss = 0.31931235\n",
      "Iteration 194, loss = 0.31705401\n",
      "Iteration 195, loss = 0.31830800\n",
      "Iteration 196, loss = 0.31626843\n",
      "Iteration 197, loss = 0.31659723\n",
      "Iteration 198, loss = 0.31526685\n",
      "Iteration 199, loss = 0.31382752\n",
      "Iteration 200, loss = 0.31447323\n",
      "  Number of Hidden Layers  Accuracy\n",
      "0                (15, 15)  0.809441\n",
      "1                (20, 20)  0.825758\n",
      "2                (25, 25)  0.811772\n",
      "3                (30, 30)  0.824592\n",
      "4                (50, 50)  0.822261\n",
      "5              (100, 100)  0.823427\n",
      "6                (70, 60)  0.828671\n",
      "7            (80, 60, 50)  0.816434\n",
      "Classification Report for (15, 15) hidden layers and neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.73      0.79       856\n",
      "           1       0.77      0.89      0.82       860\n",
      "\n",
      "    accuracy                           0.81      1716\n",
      "   macro avg       0.82      0.81      0.81      1716\n",
      "weighted avg       0.82      0.81      0.81      1716\n",
      "\n",
      "Classification Report for (20, 20) hidden layers and neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.77      0.82       856\n",
      "           1       0.79      0.88      0.84       860\n",
      "\n",
      "    accuracy                           0.83      1716\n",
      "   macro avg       0.83      0.83      0.83      1716\n",
      "weighted avg       0.83      0.83      0.83      1716\n",
      "\n",
      "Classification Report for (25, 25) hidden layers and neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.69      0.79       856\n",
      "           1       0.75      0.93      0.83       860\n",
      "\n",
      "    accuracy                           0.81      1716\n",
      "   macro avg       0.83      0.81      0.81      1716\n",
      "weighted avg       0.83      0.81      0.81      1716\n",
      "\n",
      "Classification Report for (30, 30) hidden layers and neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.76      0.81       856\n",
      "           1       0.79      0.89      0.84       860\n",
      "\n",
      "    accuracy                           0.82      1716\n",
      "   macro avg       0.83      0.82      0.82      1716\n",
      "weighted avg       0.83      0.82      0.82      1716\n",
      "\n",
      "Classification Report for (50, 50) hidden layers and neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.75      0.81       856\n",
      "           1       0.78      0.90      0.84       860\n",
      "\n",
      "    accuracy                           0.82      1716\n",
      "   macro avg       0.83      0.82      0.82      1716\n",
      "weighted avg       0.83      0.82      0.82      1716\n",
      "\n",
      "Classification Report for (100, 100) hidden layers and neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       856\n",
      "           1       0.80      0.86      0.83       860\n",
      "\n",
      "    accuracy                           0.82      1716\n",
      "   macro avg       0.82      0.82      0.82      1716\n",
      "weighted avg       0.82      0.82      0.82      1716\n",
      "\n",
      "Classification Report for (70, 60) hidden layers and neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.73      0.81       856\n",
      "           1       0.78      0.92      0.84       860\n",
      "\n",
      "    accuracy                           0.83      1716\n",
      "   macro avg       0.84      0.83      0.83      1716\n",
      "weighted avg       0.84      0.83      0.83      1716\n",
      "\n",
      "Classification Report for (80, 60, 50) hidden layers and neurons:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.81       856\n",
      "           1       0.80      0.85      0.82       860\n",
      "\n",
      "    accuracy                           0.82      1716\n",
      "   macro avg       0.82      0.82      0.82      1716\n",
      "weighted avg       0.82      0.82      0.82      1716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_2 = [(15,15),(20,20),(25,25),(30, 30), (50, 50), (100, 100),(70,60),(80,60,50)]\n",
    "accuracy_score_test_2 = []\n",
    "#conf_matrices_2 = []\n",
    "class_reports_2 = []\n",
    "for i in test_2:\n",
    "    mlp_test_2 = MLPClassifier(hidden_layer_sizes=(i), activation='logistic', alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.2, verbose=True)\n",
    "    mlp_test_2.fit(X_train,y_train)\n",
    "    predictions = mlp_test_2.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_score_test_2.append((i,accuracy))\n",
    "    #conf_matrices_2.append((i, confusion_matrix(y_test, predictions)))\n",
    "    class_reports_2.append((i, classification_report(y_test, predictions)))\n",
    "ac_2 = pd.DataFrame(accuracy_score_test_2, columns=['Number of Hidden Layers', 'Accuracy'])\n",
    "print(ac_2)\n",
    "#for neurons, matrix in conf_matrices_2:\n",
    "    #print(f\"Confusion Matrix for {neurons} neurons:\")\n",
    "    #print(matrix)\n",
    "for neurons, report in class_reports_2:\n",
    "    print(f\"Classification Report for {neurons} hidden layers and neurons:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65264a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Learning Rate  Accuracy\n",
      "0          0.001  0.748252\n",
      "1          0.010  0.769231\n",
      "2          0.050  0.798368\n",
      "3          0.200  0.828671\n",
      "4          0.500  0.864219\n",
      "5          0.600  0.853147\n",
      "6          0.800  0.854895\n",
      "Classification Report for 0.001 learning rate:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       856\n",
      "           1       0.74      0.77      0.76       860\n",
      "\n",
      "    accuracy                           0.75      1716\n",
      "   macro avg       0.75      0.75      0.75      1716\n",
      "weighted avg       0.75      0.75      0.75      1716\n",
      "\n",
      "Classification Report for 0.01 learning rate:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.71      0.75       856\n",
      "           1       0.74      0.83      0.78       860\n",
      "\n",
      "    accuracy                           0.77      1716\n",
      "   macro avg       0.77      0.77      0.77      1716\n",
      "weighted avg       0.77      0.77      0.77      1716\n",
      "\n",
      "Classification Report for 0.05 learning rate:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79       856\n",
      "           1       0.77      0.85      0.81       860\n",
      "\n",
      "    accuracy                           0.80      1716\n",
      "   macro avg       0.80      0.80      0.80      1716\n",
      "weighted avg       0.80      0.80      0.80      1716\n",
      "\n",
      "Classification Report for 0.2 learning rate:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.73      0.81       856\n",
      "           1       0.78      0.92      0.84       860\n",
      "\n",
      "    accuracy                           0.83      1716\n",
      "   macro avg       0.84      0.83      0.83      1716\n",
      "weighted avg       0.84      0.83      0.83      1716\n",
      "\n",
      "Classification Report for 0.5 learning rate:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.80      0.86       856\n",
      "           1       0.82      0.93      0.87       860\n",
      "\n",
      "    accuracy                           0.86      1716\n",
      "   macro avg       0.87      0.86      0.86      1716\n",
      "weighted avg       0.87      0.86      0.86      1716\n",
      "\n",
      "Classification Report for 0.6 learning rate:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.85       856\n",
      "           1       0.82      0.90      0.86       860\n",
      "\n",
      "    accuracy                           0.85      1716\n",
      "   macro avg       0.86      0.85      0.85      1716\n",
      "weighted avg       0.86      0.85      0.85      1716\n",
      "\n",
      "Classification Report for 0.8 learning rate:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       856\n",
      "           1       0.85      0.87      0.86       860\n",
      "\n",
      "    accuracy                           0.85      1716\n",
      "   macro avg       0.86      0.85      0.85      1716\n",
      "weighted avg       0.86      0.85      0.85      1716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vange\\AnacondaNew\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_3 = [0.001,0.01, 0.05, 0.2, 0.5, 0.6, 0.8]\n",
    "accuracy_score_test_3 = []\n",
    "class_reports_3 = []\n",
    "for i in test_3:\n",
    "    mlp_test_3 = MLPClassifier(hidden_layer_sizes=(70,60), activation='logistic', alpha=1e-4,\n",
    "                        solver='sgd', tol=1e-4, random_state=1,\n",
    "                        learning_rate_init=i, verbose=False)\n",
    "    mlp_test_3.fit(X_train, y_train)\n",
    "    predictions = mlp_test_3.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_score_test_3.append((i,accuracy))\n",
    "    class_reports_3.append((i, classification_report(y_test, predictions)))\n",
    "ac_3 = pd.DataFrame(accuracy_score_test_3, columns=['Learning Rate', 'Accuracy'])\n",
    "print(ac_3)\n",
    "for act, report in class_reports_3:\n",
    "    print(f\"Classification Report for {act} learning rate:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8d9c9b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47842169\n",
      "Iteration 2, loss = 0.43191478\n",
      "Iteration 3, loss = 0.42028978\n",
      "Iteration 4, loss = 0.40667085\n",
      "Iteration 5, loss = 0.39863893\n",
      "Iteration 6, loss = 0.38742506\n",
      "Iteration 7, loss = 0.38048112\n",
      "Iteration 8, loss = 0.37303096\n",
      "Iteration 9, loss = 0.36212901\n",
      "Iteration 10, loss = 0.36054495\n",
      "Iteration 11, loss = 0.35023979\n",
      "Iteration 12, loss = 0.33723047\n",
      "Iteration 13, loss = 0.33335934\n",
      "Iteration 14, loss = 0.32928997\n",
      "Iteration 15, loss = 0.32395615\n",
      "Iteration 16, loss = 0.32506729\n",
      "Iteration 17, loss = 0.30505809\n",
      "Iteration 18, loss = 0.31973470\n",
      "Iteration 19, loss = 0.31145958\n",
      "Iteration 20, loss = 0.29598548\n",
      "Iteration 21, loss = 0.29085641\n",
      "Iteration 22, loss = 0.29104925\n",
      "Iteration 23, loss = 0.27882131\n",
      "Iteration 24, loss = 0.27223375\n",
      "Iteration 25, loss = 0.27556864\n",
      "Iteration 26, loss = 0.26410757\n",
      "Iteration 27, loss = 0.25740577\n",
      "Iteration 28, loss = 0.24247990\n",
      "Iteration 29, loss = 0.24554964\n",
      "Iteration 30, loss = 0.24728003\n",
      "Iteration 31, loss = 0.22678240\n",
      "Iteration 32, loss = 0.24022948\n",
      "Iteration 33, loss = 0.23022709\n",
      "Iteration 34, loss = 0.21308450\n",
      "Iteration 35, loss = 0.25097704\n",
      "Iteration 36, loss = 0.24675221\n",
      "Iteration 37, loss = 0.22615298\n",
      "Iteration 38, loss = 0.20286379\n",
      "Iteration 39, loss = 0.21804156\n",
      "Iteration 40, loss = 0.19727166\n",
      "Iteration 41, loss = 0.20834059\n",
      "Iteration 42, loss = 0.19915182\n",
      "Iteration 43, loss = 0.19024096\n",
      "Iteration 44, loss = 0.19554076\n",
      "Iteration 45, loss = 0.19024307\n",
      "Iteration 46, loss = 0.18842177\n",
      "Iteration 47, loss = 0.19730124\n",
      "Iteration 48, loss = 0.18006057\n",
      "Iteration 49, loss = 0.20077091\n",
      "Iteration 50, loss = 0.18487068\n",
      "Iteration 51, loss = 0.17393330\n",
      "Iteration 52, loss = 0.16857651\n",
      "Iteration 53, loss = 0.19173870\n",
      "Iteration 54, loss = 0.18466694\n",
      "Iteration 55, loss = 0.18836245\n",
      "Iteration 56, loss = 0.15732493\n",
      "Iteration 57, loss = 0.15624781\n",
      "Iteration 58, loss = 0.16307768\n",
      "Iteration 59, loss = 0.15694146\n",
      "Iteration 60, loss = 0.14572307\n",
      "Iteration 61, loss = 0.13563014\n",
      "Iteration 62, loss = 0.14874086\n",
      "Iteration 63, loss = 0.15902458\n",
      "Iteration 64, loss = 0.16105095\n",
      "Iteration 65, loss = 0.15145091\n",
      "Iteration 66, loss = 0.16142766\n",
      "Iteration 67, loss = 0.18411650\n",
      "Iteration 68, loss = 0.18924619\n",
      "Iteration 69, loss = 0.17578916\n",
      "Iteration 70, loss = 0.21155638\n",
      "Iteration 71, loss = 0.18160681\n",
      "Iteration 72, loss = 0.18343073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.844988344988345"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(70,60), activation='relu', alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=0.5, verbose=True)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21f04541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844988344988345"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = mlp.predict(X_test)\n",
    "accuracy_score(y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81262a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.84       856\n",
      "           1       0.81      0.90      0.85       860\n",
      "\n",
      "    accuracy                           0.84      1716\n",
      "   macro avg       0.85      0.84      0.84      1716\n",
      "weighted avg       0.85      0.84      0.84      1716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4f2dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[675 181]\n",
      " [ 85 775]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8483801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
